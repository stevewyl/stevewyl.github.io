<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>DL炼丹房</title>
  
  
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://stevewyl.github.io/"/>
  <updated>2017-09-09T15:42:45.654Z</updated>
  <id>https://stevewyl.github.io/</id>
  
  <author>
    <name>Yilei Wang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Keras之文本分类实现</title>
    <link href="https://stevewyl.github.io/2017/09/09/keras/"/>
    <id>https://stevewyl.github.io/2017/09/09/keras/</id>
    <published>2017-09-09T11:41:01.000Z</published>
    <updated>2017-09-09T15:42:45.654Z</updated>
    
    <content type="html"><![CDATA[<h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>从优达DLND毕业后，一直想自己动手做点什么来着，互助班的导师也鼓励自己动手写点心得体验啥的。之前一直没怎么观看Youtube网红Siraj老师的课程视频，他每个视频最后都会有一个编程挑战。于是，想着先从自己熟悉的内容着手吧，Siraj老师第三周的编程挑战是做一个多类别的文本分类器，链接在此：<a href="https://github.com/llSourcell/How_to_do_Sentiment_Analysis" target="_blank" rel="external">Github</a>，那就来试试吧。除了想自己练练手外，也顺便把模型都好好梳理一遍。为了给自己增加些难度，是否有可能把过去几年内那些大牛们论文中的模型复现出来呢？阅读这篇文章，需要你对自然语言处理和深度学习的模型有一个基础的了解哦！</p><p>另外，需要声明的是，本文在写作过程中或多或少参考了如下大牛们的博客：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/25928551" target="_blank" rel="external">用深度学习（CNN RNN Attention）解决大规模文本分类问题 - 综述和实践</a></li><li><a href="http://www.jeyzhang.com/cnn-apply-on-modelling-sentence.html" target="_blank" rel="external">卷积神经网络(CNN)在句子建模上的应用</a></li><li><a href="http://blog.csdn.net/u010223750/article/details/51437854" target="_blank" rel="external">深度学习在文本分类中的应用</a></li><li><a href="http://blog.csdn.net/liuchonge/article/details/77140719" target="_blank" rel="external">深度学习与文本分类总结第一篇–常用模型总结</a></li><li><a href="https://cn.udacity.com/course/deep-learning-nanodegree-foundation--nd101" target="_blank" rel="external">优达学城深度学习基石纳米学位课程</a></li></ol><h2 id="文本多分类"><a href="#文本多分类" class="headerlink" title="文本多分类"></a>文本多分类</h2><p>首先我们来看下数据集长什么样子吧 :P Let‘s get started!</p><p>我们使用pandas来加载数据，数据集来自IGN.com，收集了过去20年各大游戏厂商发布的游戏数据，如发布日期，发布平台，游戏评价等变量，这里有一篇关于这个数据集很不错的分析教程 <a href="https://www.kaggle.com/ash316/20-years-of-games-analysis" target="_blank" rel="external">Kaggle</a> 。而我们现在想分析下游戏名与用户评价之间的关系，看上去并不合理，我们姑且按照Siraj老师的任务来试试。于是，游戏名作为文本变量将作为模型的输入X，而用户评价词作为文本类别Y。<br>然后来看看各个类别的数量，为了避免类别样本数的不平衡，我们这里把关于评价为Disaster的游戏去除。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">df = pd.read_csv(<span class="string">'ign.csv'</span>).iloc[:, <span class="number">1</span>:<span class="number">3</span>]</div><div class="line">df.score_phrase.value_counts()</div><div class="line">df = df[df.score_phrase != <span class="string">'Disaster'</span>]</div></pre></td></tr></table></figure><p>首先我们先来试试传统机器学习模型对文本分类任务常见的做法吧</p><h3 id="传统文本分类方法"><a href="#传统文本分类方法" class="headerlink" title="传统文本分类方法"></a>传统文本分类方法</h3><ul><li><p>词袋模型</p><p>由于计算机只能处理数字型的变量，并不能直接把文本丢给计算机然后让它告诉我们这段文字的类别。于是，我们需要对词进行one-hot编码。假设我们总共有N个词，然后对词进行索引编码并构造一个N维零向量，如果这个文本中的某些词出现，就在该词索引值位置标记为1，表示这个文本包含这个词。于是，我们会得到如下类似的向量</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">( 0, 0, 1, 0, .... , 1, ... 0, 0, 1, 0)</div></pre></td></tr></table></figure><p>但是，一般来说词库量至少都是百万级别，因此词袋模型有个两个最大的问题：高维度、高稀疏性。这种表示方法还存在一个重要的问题就是”词汇鸿沟”现象：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否有关系。</p></li><li><p>共现矩阵</p><p>为了使用上下文来表示单词间的关系，也有人提出使用基于窗口大小的共现矩阵，但仍然存在数据维度大稀疏的问题。</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/57232353.jpg" alt=""></p></li><li><p>TF-IDF</p><p>TF-IDF 用以评估一字词对于一个文档集或一个语料库中的其中一份文档的重要程度，是一种计算特征权重的方法。核心思想即，字词的重要性随着它在文档中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。有效地规避了那些高频却包含很少信息量的词。我们这里也是用TF-IDF 对文本变量进行特征提取。</p></li><li><p>分类器</p><p>分类器就是常见的机器学习分类模型了，常用的有以下两种，这里我不再赘述这两个模型的原理了。</p><ul><li>朴素贝叶斯：从垃圾邮件识别应用开始被广泛使用</li><li>支持向量机：这篇<a href="http://www.linkedin.com/pulse/please-explain-support-vector-machines-svm-like-i-am-5-joni-salminen" target="_blank" rel="external">文章</a> 很通俗地解释了SVM的工作原理 </li></ul></li></ul><p>使用Scikit-Learn库能够傻瓜似的来实现你的机器学习模型，我们这里使用TfidfVectorizer函数对文本进行特征处理，并去除停用词，模型有多类别朴素贝叶斯和线性SVM分类器。结果很不令人满意，NB模型结果稍好，准确率为28%，领先SVM 1%。下面我们来看看深度学习模型强大的性能。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">vect = TfidfVectorizer(stop_words=<span class="string">'english'</span>, token_pattern=<span class="string">r'\b\w&#123;2,&#125;\b'</span>,</div><div class="line">                       min_df=<span class="number">1</span>, max_df=<span class="number">0.1</span>, ngram_range=(<span class="number">1</span>,<span class="number">2</span>))</div><div class="line">mnb = MultinomialNB(alpha=<span class="number">2</span>)</div><div class="line">svm = SGDClassifier(loss=<span class="string">'hinge'</span>, penalty=<span class="string">'l2'</span>, alpha=<span class="number">1e-3</span>, max_iter=<span class="number">5</span>, random_state=<span class="number">42</span>)</div><div class="line">mnb_pipeline = make_pipeline(vect, mnb)</div><div class="line">svm_pipeline = make_pipeline(vect, svm)</div><div class="line">mnb_cv = cross_val_score(mnb_pipeline, title, label, scoring=<span class="string">'accuracy'</span>, cv=<span class="number">10</span>, n_jobs=<span class="number">-1</span>)</div><div class="line">svm_cv = cross_val_score(svm_pipeline, title, label, scoring=<span class="string">'accuracy'</span>, cv=<span class="number">10</span>, n_jobs=<span class="number">-1</span>)</div><div class="line">print(<span class="string">'\nMultinomialNB Classifier\'s Accuracy: %0.5f\n'</span> % mnb_cv.mean())</div><div class="line">print(<span class="string">'\nSVM Classifier\'s Accuracy: %0.5f\n'</span> % svm_cv.mean())</div></pre></td></tr></table></figure><h3 id="走进NLP和DL"><a href="#走进NLP和DL" class="headerlink" title="走进NLP和DL"></a>走进NLP和DL</h3><p>传统方法对于文本的特征表达能力很弱，神经网络同样不擅长处理这样高维度高稀疏性的数据，因此我们需要对文本做进一步的特征处理，这里就要讲到词嵌入的概念了。深度学习模型中，一个单词常常用一个低维且稠密的向量来表示，如下所示：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">( 0.286, 0.792, -0.177, -0.107, .... , 0.109, ... 0.349, 0.271, -0.642)</div></pre></td></tr></table></figure><ul><li><p>词向量</p><p>主流的词嵌入实现方法有Mikolov的<a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="external">Word2Vec</a>和斯坦福大学的<a href="https://nlp.stanford.edu/pubs/glove.pdf" target="_blank" rel="external">Glove</a>，也有人做过<a href="https://rare-technologies.com/making-sense-of-word2vec/" target="_blank" rel="external">实验</a>来比较这两种方法的优劣，并没有多大的差异。Word2Vec是基于预测的词向量模型，简单来说，就是给定一个词，去预测这个词周围可能出现的词，或者给定一些词来确定位于中心位置的词。而Glove是基于统计方法的，通过对词-词共现矩阵里的非零元素进行训练。总体来说，Word2Vec使用的是局部信息，而Glove使用的是全局信息，因此前者训练起来更慢但不占用内存，而Glove通过更多的计算资源换取训练速度上的提升。具体的实现细节可以参考这两篇论文。本文中将使用预训练的Glove300维词向量和由自己文本生成的词向量。</p></li></ul><p>为了快速实现模型，这篇文章中将使用Keras（TensorFlow的高级API）来完成。要使用Keras前，你必须安装TensorFlow作为其后端，Keras目前支持Tensorflow、Theano和CNTK作为后端，不过我还是推荐大家使用TensorFlow。Keras的中文<a href="http://keras-cn.readthedocs.io/en/latest/for_beginners/keras_linux/" target="_blank" rel="external">文档</a>能够帮助大家无坑完成安装过程。</p><p>我们先来了解一些基础的深度学习模型吧！</p><ul><li><p>CNN </p><p>深度学习入门必学的两大模型之一卷积神经网络。首先我们来理解下什么是卷积操作？卷积，你可以把它想象成一个应用在矩阵上的滑动窗口函数。下图中左边的矩阵表示的是一张黑白图像。每个方格代表了一个像素，0表示黑色，1表示白色。这个滑动窗口称作kernel或者filter。这里我们使用的是一个3*3的filter，将它的值和与其对应的原图像矩阵进行相乘，然后再将它们相加。这样我们在整个原图矩阵上滑动filter来遍历所有像素后得到一个完整的卷积特征。</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/19849125.jpg" alt=""></p><p>卷积网络也就是对输入样本进行多次卷积操作，提取数据中的局部位置的特征，然后再拼接池化层（图中的Pooling层）做进一步的降维操作，最后与全连接层拼接完成对输入样本的全新的特征构造，将新的特征向量输送给分类器（以图片分类为例）进行预测分类。我们可以把CNN类比N-gram模型，N-gram也是基于词窗范围这种局部的方式对文本进行特征提取，与CNN的做法很类似，在下文中，我们再来看看如何运用CNN对文本数据进行建模。</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/73075953.jpg" alt=""></p><p>卷积网络开始崭露头角是在CV领域，2012年的ImageNet竞赛中，大大降低了图片分类的错误率。为什么CNN在计算机视觉领域这么厉害呢？直观的感受就是：</p><ul><li>它能够学习识别基本的直线，曲线，然后是形状，点块，然后是图片中更复杂的物体。最终CNN分类器把这些大的，复杂的物体综合起来识别图片</li><li>在下图中的例子中，可以看作这样的层级关系：<ul><li>简单的形状，如椭圆，暗色圆圈</li><li>复杂的物体（简单形状的组合），例如眼睛，鼻子，毛发</li><li>狗的整体（复杂物体的组合）</li></ul></li></ul><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/30339409.jpg" alt=""></p></li><li><p>RNN</p><p>而循环网络与CNN不同的是，CNN学习空间位置上局部位置的特征表示，而RNN学习的是时间顺序上的特征，用来处理序列数据，如股价，文本等。RNN之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点是相互连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。</p><p>就像我们说话一样，我们不能把说的话倒过来表示，这样会变得毫无意义，并不会明白你在说什么，也就是说文本中的每个词是包含顺序信息的，由此可以使用RNN对文本数据进行建模。<br><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/18106498.jpg" alt=""></p><p>但是，随着时间的不断增加，你的隐藏层一次又一次地乘以权重W。假如某个权重w是一个接近于0或者大于1的数，随着乘法次数的增加，这个权重值会变得很小或者很大，造成反向传播时梯度计算变得很困难，造成梯度爆炸或者梯度消失的情况，模型难以训练。也就是说一般的RNN模型对于长时间距离的信息记忆很差，比如人老了会忘记某件很久发生的事情一样，于是，LSTM和GRU 应运而生。LSTM与GRU很相似，以LSTM为例。</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/54396153.jpg" alt=""></p><p>LSTM又称为长短期记忆网络，LSTM 单元乍看起来很复杂。关键的新增部分就在于标记为 C 的单元状态。在这个单元中，有四个显示为黄色框的网络层，每个层都有自己的权重，如以 σ 标记的层是 sigmoid 层。这些红圈表示逐点或逐元素操作。单元状态在通过 LSTM 单元时几乎没有交互，使得大部分信息得以保留，单元状态仅通过这些控制门（gate）进行修改。第一个控制门是遗忘门，用来决定我们会从单元状态中丢弃什么信息。第二个门是更新们，用以确定什么样的新信息被存放到单元状态中。最后一个门是输出门，我们需要确定输出什么样的值。总结来说 LSTM 单元由单元状态和一堆用于更新信息的控制门组成，让信息部分传递到隐藏层状态。更直观的来讲，把LSTM看作是一部电影，可以把单元状态看作是剧情主线，而随着剧情的发展，有些不必要的事件会被遗忘，而一些更加影响主线的剧情会被加入到单元状态中来，不断更新剧情然后输出新的剧情发展。</p></li><li><p>Attention机制</p><p>基于Attention的模型在NLP领域首先被应用于自然语言生成问题中，用于改进机器翻译任务的性能。我们这里也以机器翻译为例来解释下注意力机制的原理。我们可以把翻译任务是一个序列向另一个序列转换的过程。</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-29/69730967.jpg" alt=""></p><p>上图就是Seq2Seq模型的基本结构，由编码器（Encoder）和解码器（Decoder）组成。编码器负责将输入的单词按顺序进行信息提取，在最后一步生成的隐藏状态即固定长度的句子的特征向量。然后解码器从这个句子向量中获取信息对文本进行翻译。由于解码器的主要信息来源就是最后一步的隐藏状态，这个h3向量必须尽可能地包含句子的所有必要的信息。这个向量说白了就是句子嵌入（类比词嵌入）。假如我们需要翻译的文本不是很长，这个模型已经能达到很不错的性能。假如我们现在要翻译一句超过50个单词的句子，似乎这个模型很难再hold住，即使你在训练的时候使用了LSTM去提取句子特征，去尽可能保留过去的记忆，但还是达不到想要的结果。</p><p>而注意力机制恰恰是为了解决长距离依赖的问题，我们不再需要固定长度的句子向量，而是让解码器自己去输入文本中寻找想要关注的被翻译文本。比如把”I am learning deep learning model“成中文时，我们让解码器去与输入文本中的词对齐，翻译deep的时候去关注deep这个词，而不是平等对待每个有可能的词，找到与输入文本相对应的相同语义的词，而不再是对句子进行特征提取。</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-29/74136352.jpg" alt=""></p><p>上图我们可以看到，解码器在翻译下一个词时，需要依赖之前已经翻译好的文本和与输入文本相对齐的那个词。简单描述的话，用解码器t时刻的隐藏状态去和输入文本中的每个单词对应的隐藏状态去比对，通过某个函数f去计算带翻译的单词yi与每个输入单词对齐的可能性。而编码器由Bi-LSTM模型组成。不同的语言的f函数可能会有差别，就像中文和英文，语法结构差异很大，很难按顺序单词一一对齐。由此可以得出结论，注意力机制的核心思想是在翻译每个目标词（或对文本进行分类时）所用的上下文是不同的，这样的考虑显然是更合理的。具体实现请见这篇<a href="https://arxiv.org/pdf/1409.0473v7.pdf" target="_blank" rel="external">论文</a>。而如何将注意力机制运用到文本分类中来，下文会介绍。</p></li></ul><h3 id="深度学习文本分类模型"><a href="#深度学习文本分类模型" class="headerlink" title="深度学习文本分类模型"></a>深度学习文本分类模型</h3><ul><li><p>TextCNN</p><p>这是CNN首次被应用于文本分类任务的开山之作，可以说，之后很多论文都是基于此进行拓展的。它是由<a href="https://arxiv.org/abs/1408.5882" target="_blank" rel="external">Yoon Kim</a>于2014年发表的，你可以在github上找到各种不同深度学习框架对于这个模型的实现。下面我们来细细品读这篇论文吧。</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-29/75206008.jpg" alt=""></p><p>上图很好地诠释了模型的框架。假设我们有一句句子需要对其进行分类。句子中每个词是由n维词向量组成的，也就是说输入矩阵大小为m*n，其中m为句子长度。CNN需要对输入样本进行卷积操作，对于文本数据，filter不再横向滑动，仅仅是向下移动，有点类似于N-gram在提取词与词间的局部相关性。图中共有三种步长策略，分别是2,3,4，每个步长都有两个filter（实际训练时filter数量会很多）。在不同词窗上应用不同filter，最终得到6个卷积后的向量。然后对每一个向量进行最大化池化操作并拼接各个池化值，最终得到这个句子的特征表示，将这个句子向量丢给分类器进行分类，至此完成整个流程。</p><p>文中作者还提出了动态的词向量，即将词向量也作为权重变量进行训练，而我们平时常用的产生词向量的方法有从当前数据集中自己产生的词向量和使用预训练好的word2vec或glove词向量，都属于静态词向量范畴，即它们不再网络训练时发生变化。文中实验表明，动态的词向量表现更好。有时间的话之后来尝使用Tensorflow来实现这种动态词向量。另外，这篇论文 <a href="https://arxiv.org/abs/1510.03820" target="_blank" rel="external">A Sensitivity Analysis of (and Practitioners’ Guide to) Convolutional Neural Networks for Sentence Classification</a>详细地阐述了关于TextCNN模型的调参心得。</p></li><li><p>DCNN</p><p>这篇<a href="https://arxiv.org/abs/1404.2188" target="_blank" rel="external">论文</a>的亮点在于采用的动态的K-max Pooling，而不是我们常见的Max Pooling层。模型细节主要分为以下几个部分：</p><ol><li><p>宽卷积：卷积分为两种，窄卷积和宽卷积。窄卷积即从第一个元素开始卷积操作，这样第一个元素和最后一个元素只能被filter扫过一次。而宽卷积为了弥补这一点，就在第一个元素前和最后一个元素后增加0作为补充，因此宽卷积又叫做补零法。当filter长度相对输入向量的长度较大时，你会发现宽卷积很有用，或者说很有必要。</p></li><li><p>动态的K-Max Pooling</p><p>下图中可以看到两个个池化层的K是不确定的，即动态的，具体的取值依赖于输入和网络的其他参数。文中提到，K的取值与输入文本的长度，网络中总共的卷积数，上一层卷积的个数有关，具体可以查看论文（始终没有解决数学公式的显示问题，所以不展开了）。因此文中的pooling的结果不是返回一个最大值，而是返回k组最大值，这些最大值是原输入的一个子序列</p></li><li><p>Folding层</p><p>图中很清楚地看到，Folding层将词向量的维度缩减一半，即将两行的向量相加，可能考虑相邻两行之前某种未知的联系吧。不确定是否真的有效，第一次看到卷积网络中出现这样缩减维度的层。</p></li></ol><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-9-6/53207281.jpg" alt=""></p></li><li><p>Bi-LSTM</p><p>本篇<a href="https://www.ijcai.org/Proceedings/16/Papers/408.pdf" target="_blank" rel="external">论文</a>由复旦大学的邱锡鹏教授的团队于2015年发表，文中详细地阐述了RNN模型用于文本分类任务的各种变体模型。最简单的RNN用于文本分类如下图所示，这是LSTM用于网络结构原理示意图，示例中的是利用最后一个词的结果直接接全连接层softmax输出就完成了。详情可见这里的阅读笔记<a href="https://zhuanlan.zhihu.com/p/27562717?refer=xitucheng10" target="_blank" rel="external">链接</a></p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-9-8/67010515.jpg" alt=""></p></li><li><p>CLSTM </p><p>论文<a href="https://arxiv.org/abs/1511.08630" target="_blank" rel="external">A C-LSTM Neural Network for Text Classification</a>中将CNN和RNN混合使用作为文本的分类器。其实就是将CNN训练得到的新的特征作为LSTM的输入，模型的简单描述如下：</p><ul><li>Feature maps指不同词窗经过不同过滤层即卷积操作后得到的特征集合</li><li>Window feature sequence是指CNN不再经过Max-pooling操作，而是将特征集合重新排列，得到同一词窗在经过不同卷积操作后的综合特征向量，即把相同颜色的放在一个序列里面，然后依次排列下来</li><li>在window feature sequence层的每个序列，其实和原始句子中的序列是对应的，保持了原有的相对顺序，只不过是中间进行了卷积的操作，将这些新的向量作为LSTM的输入变量</li></ul><p>​</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-9-7/9330995.jpg" alt=""></p></li><li><p>RCNN</p><p>这篇<a href="http://www.nlpr.ia.ac.cn/cip/~liukang/liukangPageFile/Recurrent%20Convolutional%20Neural%20Networks%20for%20Text%20Classification.pdf" target="_blank" rel="external">Recurrent Convolutional Neural Networks for Text Classification</a>是由中科院与2015年发表在AAAI上的一篇文章。文中将RNN和CNN以另外一种方式呈现。<img src="http://ovcg0j4b2.bkt.clouddn.com/17-9-7/9080347.jpg" alt=""></p><p>我们可以发现，这个模型是把CNN模型中的卷积的部分使用RNN代替了，最后加上池化层。而这个RNN层做的事情是，将每一个词分别和左边的词以及右边的词进行融合。每以文本先经过1层双向LSTM，该词的左侧的词正向输入进去得到一个词向量，该词的右侧反向输入进去得到一个词向量。再结合该词的词向量，生成一个 3k维的组合词向量。然后再将这些新的词向量传入全连接层，紧接着是最大化池化层进行特征降维。最后接上全连接层，便完成多分类任务。</p></li><li><p>FastText</p><p>FastText是Facebook于2016年发表的论文中提出的一种简单快速实现的 文本分类模型。可能你已经被前面那些复杂的模型搞得七荤八素了，那么这个模型你很快地理解，令人意外的是，它的性能并不差。输入变量是经过embedding的词向量，这里的隐藏层只是一个简单的平均池化层，然后把这个池化过的向量丢给softmax分类器就完成了。另外，这里的X并不仅仅是单个单词，也可以加入N-gram组合的词作为输入的一部分，文中将2-元和3-元的特征也加入到了模型中。本文的思想在于通过简单的特征线性组合就可以达到不错的分类性能，我们可以把fasttext当作是工业界一种快速实现模型的产物。</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-29/18150827.jpg" alt=""></p></li><li><p>Hierarchical Attention Networks(HAN)</p><p><a href="http://aclweb.org/anthology/N/N16/N16-1174.pdf" target="_blank" rel="external">本文</a>最大的特点是结合了注意力机制，并成功运用到文本分类任务中。模型如下图所示，分为两大部分，分别是对句子建模和对文档建模。之前提到的模型基本上都是在对句子进行建模，通过对句子中的词进行特征组合，形成句子向量。本文更进一步的是，对句子的上一级篇章进行建模。我们假设评论中有好几句话，那么我们首先要切分句子然后再切分词，对于长评论的分类是一个不错的选择。</p><p>首先，词向量会经过双向LSTM网络完成编码，将隐藏层的输出和注意力机制相结合，形成对句子的特征表示。然后每一个句子相当于一个词，再重复一次前一步词到句子的建模过程，完成句子到文档的建模过程。而注意力机制在这里发挥的作用相当于去寻找这句句子中的核心词或者这篇文档中的核心句子。具体实现的过程可参照我接下来的代码。<a href="https://github.com/fchollet/keras/issues/4962#issuecomment-272888992" target="_blank" rel="external">这里</a>有关于在Keras中完成Attention层的构建的详细讨论。游戏标题只涉及单句，因此构不成文档，只需要 word-level 这一层的注意力即可。加入Attention之后最大的好处自然是能够直观的解释各个句子和词对分类类别的重要性。</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-9-7/6359712.jpg" alt=""></p></li><li><p>Aspect Level Sentiment</p><p>首先来介绍先aspect的概念，给定一个句子和句子中出现的某个aspect，aspect-level 情感分析的目标是分析出这个句子在给定aspect上的情感倾向。例如：”Great food but the service was dreadful!” 在“food”这个aspect上，情感倾向为正，而在 “service”这个aspect上情感倾向为负。Aspect level的情感分析相对于文档级别来说粒度更细。</p><p>这里有两篇阅读笔记比较详细地描述了两篇相关论文：</p><ol><li><a href="https://zhuanlan.zhihu.com/p/22841142" target="_blank" rel="external">Aspect Level Sentiment Classification with Deep Memory Network</a></li><li><a href="https://zhuanlan.zhihu.com/p/23615176" target="_blank" rel="external">Attention-based LSTM for Aspect-level Sentiment Classification</a></li></ol></li></ul><h3 id="实战！"><a href="#实战！" class="headerlink" title="实战！"></a>实战！</h3><p>首先我们先要对文本数据进行编码，因为模型只能接受数值型的数据。常见的编码之前有提到过有One-Hot和词嵌入。第一步，来划分训练样本和测试样本。如果需要将你的模型部署到产品中去，则需要更加复杂的划分，详情请见吴恩达最新的AI<a href="https://www.deeplearning.ai/" target="_blank" rel="external">课程</a>中提及的机器学习项目中必须注意的一些问题。这里，我们不需要考虑太多的细节，就简单划分训练集和测试集即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入使用到的库</span></div><div class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</div><div class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</div><div class="line"><span class="keyword">from</span> keras.layers.merge <span class="keyword">import</span> concatenate</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential, Model</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Embedding, Activation, merge, Input, Lambda, Reshape</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Convolution1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> LSTM, GRU, TimeDistributed, Bidirectional</div><div class="line"><span class="keyword">from</span> keras.utils.np_utils <span class="keyword">import</span> to_categorical</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> initializers</div><div class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</div><div class="line"><span class="keyword">from</span> keras.engine.topology <span class="keyword">import</span> Layer</div><div class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</div><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># 划分训练/测试集</span></div><div class="line">X_train, X_test, y_train, y_test = train_test_split(title, label, test_size=<span class="number">0.1</span>, random_state=<span class="number">42</span>)</div><div class="line"></div><div class="line"><span class="comment"># 对类别变量进行编码，共10类</span></div><div class="line">y_labels = list(y_train.value_counts().index)</div><div class="line">le = preprocessing.LabelEncoder()</div><div class="line">le.fit(y_labels)</div><div class="line">num_labels = len(y_labels)</div><div class="line">y_train = to_categorical(y_train.map(<span class="keyword">lambda</span> x: le.transform([x])[<span class="number">0</span>]), num_labels)</div><div class="line">y_test = to_categorical(y_test.map(<span class="keyword">lambda</span> x: le.transform([x])[<span class="number">0</span>]), num_labels)</div><div class="line"></div><div class="line"><span class="comment"># 分词，构建单词-id词典       </span></div><div class="line">tokenizer = Tokenizer(filters=<span class="string">'!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`&#123;|&#125;~\t\n'</span>,lower=<span class="keyword">True</span>,split=<span class="string">" "</span>)</div><div class="line">tokenizer.fit_on_texts(title)</div><div class="line">vocab = tokenizer.word_index</div><div class="line"></div><div class="line"><span class="comment"># 将每个词用词典中的数值代替</span></div><div class="line">X_train_word_ids = tokenizer.texts_to_sequences(X_train)</div><div class="line">X_test_word_ids = tokenizer.texts_to_sequences(X_test)</div><div class="line"></div><div class="line"><span class="comment"># One-hot</span></div><div class="line">x_train = tokenizer.sequences_to_matrix(X_train_word_ids, mode=<span class="string">'binary'</span>)</div><div class="line">x_test = tokenizer.sequences_to_matrix(X_test_word_ids, mode=<span class="string">'binary'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 序列模式</span></div><div class="line">x_train = pad_sequences(X_train_word_ids, maxlen=<span class="number">20</span>)</div><div class="line">x_test = pad_sequences(X_test_word_ids, maxlen=<span class="number">20</span>)</div></pre></td></tr></table></figure><p>Keras提供两大类模型框架。第一种是Sequential模式，就像搭积木一样，将你想要的网络层拼接起来，可以理解为串联。而另外一种是Model模式，需要你指定模型的输入和输出格式，更加灵活地组合你的网络层，可以理解为串联加并联。搭建完模型结构后，你需要对模型进行编译，这一步你需要指定模型的损失函数，本文是文本多分类任务，所以损失函数是多类别的交叉熵函数。另外， 需要确定损失函数的优化算法和模型评估指标。Adam优化器是目前公认的各项任务中性能最优的，所以本文将全部使用Adam作为优化器。这里有一篇实战<a href="https://blog.slinuxer.com/2016/09/sgd-comparison" target="_blank" rel="external">教程</a>对不同优化器做了性能评估。接着就是模型的训练，使用fit函数，这里需要指定的参数有输入数据，批量大小，迭代轮数，验证数据集等。然后，你就能看到你的模型开始愉快地运行起来了。Keras作为Tensforflow的高级API，对很多细节进行了封装，可以让深度学习小白快速上手，如果你需要实现更加复杂的模型的话，就需要去好好研究下Tensorflow了。下一篇博文目标是用Tensorflow来实现简单的机器翻译任务，也为接下来准备参加的AI Challenger比赛做准备！</p><ul><li><p>One-Hot + MLP</p><p>MLP翻译过来叫多层感知机，其实就是多隐藏层的网络。如此简单的模型，结果居然出奇的好！卖个关子，最后看看各个模型的最终准确率排名 = =</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">model = Sequential()</div><div class="line"><span class="comment"># 全连接层</span></div><div class="line">model.add(Dense(<span class="number">512</span>, input_shape=(len(vocab)+<span class="number">1</span>,), activation=<span class="string">'relu'</span>))</div><div class="line"><span class="comment"># DropOut层</span></div><div class="line">model.add(Dropout(<span class="number">0.5</span>))</div><div class="line"><span class="comment"># 全连接层+分类器</span></div><div class="line">model.add(Dense(num_labels,activation=<span class="string">'softmax'</span>))</div><div class="line"></div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</div><div class="line">              optimizer=<span class="string">'adam'</span>,</div><div class="line">              metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line">model.fit(x_train, y_train,</div><div class="line">          batch_size=<span class="number">32</span>,</div><div class="line">          epochs=<span class="number">15</span>,</div><div class="line">          validation_data=(x_test, y_test))</div></pre></td></tr></table></figure></li><li><p>CNN</p><ul><li><p>模仿LeNet-5</p><p>LeNet-5是卷积神经网络的作者Yann LeCun用于MNIST识别任务提出的模型。模型很简单，就是卷积池化层的堆叠，最后加上几层全连接层。我们依样画葫芦，将它运用在文本分类任务中，只是模型的输入不同。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 模型结构：嵌入-卷积池化*2-dropout-BN-全连接-dropout-全连接</span></div><div class="line">model.add(Embedding(len(vocab)+<span class="number">1</span>, <span class="number">300</span>, input_length=<span class="number">20</span>))</div><div class="line">model.add(Convolution1D(<span class="number">256</span>, <span class="number">3</span>, padding=<span class="string">'same'</span>))</div><div class="line">model.add(MaxPool1D(<span class="number">3</span>,<span class="number">3</span>,padding=<span class="string">'same'</span>))</div><div class="line">model.add(Convolution1D(<span class="number">128</span>, <span class="number">3</span>, padding=<span class="string">'same'</span>))</div><div class="line">model.add(MaxPool1D(<span class="number">3</span>,<span class="number">3</span>,padding=<span class="string">'same'</span>))</div><div class="line">model.add(Convolution1D(<span class="number">64</span>, <span class="number">3</span>, padding=<span class="string">'same'</span>))</div><div class="line">model.add(Flatten())</div><div class="line">model.add(Dropout(<span class="number">0.1</span>))</div><div class="line">model.add(BatchNormalization()) <span class="comment"># (批)规范化层</span></div><div class="line">model.add(Dense(<span class="number">256</span>,activation=<span class="string">'relu'</span>))</div><div class="line">model.add(Dropout(<span class="number">0.1</span>))</div><div class="line">model.add(Dense(num_labels,activation=<span class="string">'softmax'</span>))</div><div class="line"></div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</div><div class="line">              optimizer=<span class="string">'adam'</span>,</div><div class="line">              metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line">model.fit(X_train_padded_seqs, y_train,</div><div class="line">          batch_size=<span class="number">32</span>,</div><div class="line">          epochs=<span class="number">15</span>,</div><div class="line">          validation_data=(X_test_padded_seqs, y_test))</div></pre></td></tr></table></figure></li><li><p>TextCNN（这里需要使用Model模式）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 模型结构：词嵌入-卷积池化*3-拼接-全连接-dropout-全连接</span></div><div class="line">main_input = Input(shape=(<span class="number">20</span>,), dtype=<span class="string">'float64'</span>)</div><div class="line"><span class="comment"># 词嵌入（使用预训练的词向量）</span></div><div class="line">embedder = Embedding(len(vocab) + <span class="number">1</span>, <span class="number">300</span>, input_length = <span class="number">20</span>, weights = [embedding_matrix], trainable = <span class="keyword">False</span>)</div><div class="line">embed = embedder(main_input)</div><div class="line"><span class="comment"># 词窗大小分别为3,4,5</span></div><div class="line">cnn1 = Convolution1D(<span class="number">256</span>, <span class="number">3</span>, padding=<span class="string">'same'</span>, strides = <span class="number">1</span>, activation=<span class="string">'relu'</span>)(embed)</div><div class="line">cnn1 = MaxPool1D(pool_size=<span class="number">4</span>)(cnn1)</div><div class="line">cnn2 = Convolution1D(<span class="number">256</span>, <span class="number">4</span>, padding=<span class="string">'same'</span>, strides = <span class="number">1</span>, activation=<span class="string">'relu'</span>)(embed)</div><div class="line">cnn2 = MaxPool1D(pool_size=<span class="number">4</span>)(cnn2)</div><div class="line">cnn3 = Convolution1D(<span class="number">256</span>, <span class="number">5</span>, padding=<span class="string">'same'</span>, strides = <span class="number">1</span>, activation=<span class="string">'relu'</span>)(embed)</div><div class="line">cnn3 = MaxPool1D(pool_size=<span class="number">4</span>)(cnn3)</div><div class="line"><span class="comment"># 合并三个模型的输出向量</span></div><div class="line">cnn = concatenate([cnn1,cnn2,cnn3], axis=<span class="number">-1</span>)</div><div class="line">flat = Flatten()(cnn)</div><div class="line">drop = Dropout(<span class="number">0.2</span>)(flat)</div><div class="line">main_output = Dense(num_labels, activation=<span class="string">'softmax'</span>)(drop)</div><div class="line">model = Model(inputs = main_input, outputs = main_output)</div></pre></td></tr></table></figure></li><li><p>DCNN（占坑）</p></li></ul></li><li><p>RNN</p><ul><li><p>LSTM（你也可以换成GRU，经过多次试验GRU的性能较LSTM稍好）</p><p>GRU采用与LSTM相似的单元结构用于控制信息的更新与保存，它将遗忘门和输入门合成了一个单一的更新门，最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 模型结构：词嵌入-LSTM-全连接</span></div><div class="line">model = Sequential()</div><div class="line">model.add(Embedding(len(vocab)+<span class="number">1</span>, <span class="number">300</span>, input_length=<span class="number">20</span>))</div><div class="line">model.add(LSTM(<span class="number">256</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.1</span>))</div><div class="line">model.add(Dense(num_labels, activation=<span class="string">'softmax'</span>))</div></pre></td></tr></table></figure></li><li><p>Bi-GRU</p><p>需要注意的是，你如果需要堆叠多层RNN，需要在前一层返回序列，设置return_sequences参数为True即可。Bi即双向RNN结构，模型会从正向读取文本，也会逆向读取文本，从两个角度去获取文本的顺序信息。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 模型结构：词嵌入-双向GRU*2-全连接</span></div><div class="line">model = Sequential()</div><div class="line">model.add(Embedding(len(vocab)+<span class="number">1</span>, <span class="number">300</span>, input_length=<span class="number">20</span>))</div><div class="line">model.add(Bidirectional(GRU(<span class="number">256</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.1</span>, return_sequences=<span class="keyword">True</span>)))</div><div class="line">model.add(Bidirectional(GRU(<span class="number">256</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.1</span>)))</div><div class="line">model.add(Dense(num_labels, activation=<span class="string">'softmax'</span>))</div></pre></td></tr></table></figure></li></ul></li><li><p>CNN+RNN</p><ul><li><p>C-LSTM串联（将CNN的输出直接拼接上RNN）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 模型结构：词嵌入-卷积池化-GRU*2-全连接</span></div><div class="line">model = Sequential()</div><div class="line">model.add(Embedding(len(vocab)+<span class="number">1</span>, <span class="number">300</span>, input_length=<span class="number">20</span>))</div><div class="line">model.add(Convolution1D(<span class="number">256</span>, <span class="number">3</span>, padding=<span class="string">'same'</span>, strides = <span class="number">1</span>))</div><div class="line">model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">model.add(MaxPool1D(pool_size=<span class="number">2</span>))</div><div class="line">model.add(GRU(<span class="number">256</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.1</span>, return_sequences = <span class="keyword">True</span>))</div><div class="line">model.add(GRU(<span class="number">256</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.1</span>))</div><div class="line">model.add(Dense(num_labels, activation=<span class="string">'softmax'</span>))</div></pre></td></tr></table></figure></li><li><p>并联（将CNN的输出和RNN的输出合并成一个输出）（<a href="http://www.cips-cl.org/static/anthology/CCL-2016/CCL-16-004.pdf" target="_blank" rel="external">论文</a>）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 模型结构：词嵌入-卷积池化-全连接 ---拼接-全连接</span></div><div class="line"><span class="comment">#                -双向GRU-全连接</span></div><div class="line">main_input = Input(shape=(<span class="number">20</span>,), dtype=<span class="string">'float64'</span>)</div><div class="line">embed = Embedding(len(vocab)+<span class="number">1</span>, <span class="number">300</span>, input_length=<span class="number">20</span>)(main_input)</div><div class="line">cnn = Convolution1D(<span class="number">256</span>, <span class="number">3</span>, padding=<span class="string">'same'</span>, strides = <span class="number">1</span>, activation=<span class="string">'relu'</span>)(embed)</div><div class="line">cnn = MaxPool1D(pool_size=<span class="number">4</span>)(cnn)</div><div class="line">cnn = Flatten()(cnn)</div><div class="line">cnn = Dense(<span class="number">256</span>)(cnn)</div><div class="line">rnn = Bidirectional(GRU(<span class="number">256</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.1</span>))(embed)</div><div class="line">rnn = Dense(<span class="number">256</span>)(rnn)</div><div class="line">con = concatenate([cnn,rnn], axis=<span class="number">-1</span>)</div><div class="line">main_output = Dense(num_labels, activation=<span class="string">'softmax'</span>)(con)</div><div class="line">model = Model(inputs = main_input, outputs = main_output)</div></pre></td></tr></table></figure></li><li><p>RCNN</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 模型结构：词嵌入*3-LSTM*2-拼接-全连接-最大化池化-全连接</span></div><div class="line"><span class="comment"># 我们需要重新整理数据集</span></div><div class="line">left_train_word_ids = [[len(vocab)] + x[:<span class="number">-1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> X_train_word_ids]</div><div class="line">left_test_word_ids = [[len(vocab)] + x[:<span class="number">-1</span>] <span class="keyword">for</span> x <span class="keyword">in</span> X_test_word_ids]</div><div class="line">right_train_word_ids = [x[<span class="number">1</span>:] + [len(vocab)] <span class="keyword">for</span> x <span class="keyword">in</span> X_train_word_ids]</div><div class="line">right_test_word_ids = [x[<span class="number">1</span>:] + [len(vocab)] <span class="keyword">for</span> x <span class="keyword">in</span> X_test_word_ids]</div><div class="line"></div><div class="line"><span class="comment"># 分别对左边和右边的词进行编码</span></div><div class="line">left_train_padded_seqs = pad_sequences(left_train_word_ids, maxlen=<span class="number">20</span>)</div><div class="line">left_test_padded_seqs = pad_sequences(left_test_word_ids, maxlen=<span class="number">20</span>)</div><div class="line">right_train_padded_seqs = pad_sequences(right_train_word_ids, maxlen=<span class="number">20</span>)</div><div class="line">right_test_padded_seqs = pad_sequences(right_test_word_ids, maxlen=<span class="number">20</span>)</div><div class="line"></div><div class="line"><span class="comment"># 模型共有三个输入，分别是左词，右词和中心词</span></div><div class="line">document = Input(shape = (<span class="keyword">None</span>, ), dtype = <span class="string">"int32"</span>)</div><div class="line">left_context = Input(shape = (<span class="keyword">None</span>, ), dtype = <span class="string">"int32"</span>)</div><div class="line">right_context = Input(shape = (<span class="keyword">None</span>, ), dtype = <span class="string">"int32"</span>)</div><div class="line"></div><div class="line"><span class="comment"># 构建词向量</span></div><div class="line">embedder = Embedding(len(vocab) + <span class="number">1</span>, <span class="number">300</span>, input_length = <span class="number">20</span>)</div><div class="line">doc_embedding = embedder(document)</div><div class="line">l_embedding = embedder(left_context)</div><div class="line">r_embedding = embedder(right_context)</div><div class="line"></div><div class="line"><span class="comment"># 分别对应文中的公式(1)-(7)</span></div><div class="line">forward = LSTM(<span class="number">256</span>, return_sequences = <span class="keyword">True</span>)(l_embedding) <span class="comment"># 等式(1)</span></div><div class="line"><span class="comment"># 等式(2)</span></div><div class="line">backward = LSTM(<span class="number">256</span>, return_sequences = <span class="keyword">True</span>, go_backwards = <span class="keyword">True</span>)(r_embedding) </div><div class="line">together = concatenate([forward, doc_embedding, backward], axis = <span class="number">2</span>) <span class="comment"># 等式(3)</span></div><div class="line"></div><div class="line">semantic = TimeDistributed(Dense(<span class="number">128</span>, activation = <span class="string">"tanh"</span>))(together) <span class="comment"># 等式(4)</span></div><div class="line"><span class="comment"># 等式(5)</span></div><div class="line">pool_rnn = Lambda(<span class="keyword">lambda</span> x: backend.max(x, axis = <span class="number">1</span>), output_shape = (<span class="number">128</span>, ))(semantic) </div><div class="line">output = Dense(<span class="number">10</span>, activation = <span class="string">"softmax"</span>)(pool_rnn) <span class="comment"># 等式(6)和(7)</span></div><div class="line">model = Model(inputs = [document, left_context, right_context], outputs = output)</div><div class="line"></div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</div><div class="line">              optimizer=<span class="string">'adam'</span>,</div><div class="line">              metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line">model.fit([X_train_padded_seqs, left_train_padded_seqs, right_train_padded_seqs],                  y_train,</div><div class="line">           batch_size=<span class="number">32</span>,</div><div class="line">           epochs=<span class="number">12</span>,</div><div class="line">           validation_data=([X_test_padded_seqs, left_test_padded_seqs,                                              right_test_padded_seqs], y_test))</div></pre></td></tr></table></figure></li></ul></li><li><p>Attention</p><ul><li><p>HAN</p><p>由于Keras目前还没有现成的Attention层可以直接使用，我们需要自己来构建一个新的层函数。Keras自定义的函数主要分为四个部分，分别是：</p><ul><li>init：初始化一些需要的参数</li><li>bulid：具体来定义权重是怎么样的</li><li>call：核心部分，定义向量是如何进行运算的</li><li>compute_output_shape：定义该层输出的大小</li></ul><p>这里我只列出核心的call部分，传送门：<a href="https://keras.io/layers/writing-your-own-keras-layers/" target="_blank" rel="external">官方示例</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div></pre></td><td class="code"><pre><div class="line"><span class="class"><span class="keyword">class</span> <span class="title">AttLayer</span><span class="params">(Layer)</span>:</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, **kwargs)</span>:</span></div><div class="line">        self.init = initializers.get(init)</div><div class="line">        ...</div><div class="line">    </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">build</span><span class="params">(self, input_shape)</span>:</span></div><div class="line">        <span class="keyword">assert</span> len(input_shape)==<span class="number">3</span></div><div class="line">        self.W = </div><div class="line">        self.b =</div><div class="line">        self.u =</div><div class="line">        </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">call</span><span class="params">(self, x)</span>:</span></div><div class="line">        uit = K.dot(x, self.W) <span class="comment"># 对应公式(5)</span></div><div class="line">        uit = K.squeeze(uit, <span class="number">-1</span>) <span class="comment"># 对应公式(5)</span></div><div class="line">        uit = uit + self.b <span class="comment"># 对应公式(5)</span></div><div class="line">        uit = K.tanh(uit) <span class="comment"># 对应公式(5)</span></div><div class="line">        ait = uit * self.u <span class="comment"># 对应公式(6)</span></div><div class="line">        ait = K.exp(ait) <span class="comment"># 对应公式(6)</span></div><div class="line">        <span class="comment"># 对应公式(6)</span></div><div class="line">        ait /= K.cast(K.sum(ait, axis=<span class="number">1</span>, keepdims=<span class="keyword">True</span>) + K.epsilon(), K.floatx())</div><div class="line">        ait = K.expand_dims(ait) <span class="comment"># 对应公式(7)</span></div><div class="line">        weighted_input = x * ait <span class="comment"># 对应公式(7)</span></div><div class="line">        output = K.sum(weighted_input, axis=<span class="number">1</span>) <span class="comment"># 对应公式(7)</span></div><div class="line">        <span class="keyword">return</span> output</div><div class="line">      </div><div class="line">    <span class="function"><span class="keyword">def</span> <span class="title">compute_output_shape</span><span class="params">(self, input_shape)</span>:</span></div><div class="line">        <span class="keyword">return</span> (input_shape[<span class="number">0</span>], input_shape[<span class="number">-1</span>])</div></pre></td></tr></table></figure><p>定义好Attention层，直接调用即可，我们来看下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 模型结构：词嵌入-双向GRU-Attention-全连接</span></div><div class="line">inputs = Input(shape=(<span class="number">20</span>,), dtype=<span class="string">'float64'</span>)</div><div class="line">embed = Embedding(len(vocab) + <span class="number">1</span>,<span class="number">300</span>, input_length = <span class="number">20</span>)(inputs)</div><div class="line">gru = Bidirectional(GRU(<span class="number">100</span>, dropout=<span class="number">0.2</span>, return_sequences=<span class="keyword">True</span>))(embed)</div><div class="line">attention = AttLayer()(gru)</div><div class="line">output = Dense(num_labels, activation=<span class="string">'softmax'</span>)(attention)</div><div class="line">model = Model(inputs, output)</div></pre></td></tr></table></figure></li><li><p>Aspect-Level Sentiment（占坑）</p></li></ul></li><li><p>FastText（模型很简单，比较复杂的是构造输入数据）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 模型结构：词嵌入(n-gram)-最大化池化-全连接</span></div><div class="line"><span class="comment"># 生成n-gram组合的词(以3为例)</span></div><div class="line">ngram = <span class="number">3</span></div><div class="line"><span class="comment"># 将n-gram词加入到词表</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_ngram</span><span class="params">(sent, ngram_value)</span>:</span></div><div class="line"><span class="keyword">return</span> set(zip(*[sent[i:] <span class="keyword">for</span> i <span class="keyword">in</span> range(ngram_value)]))</div><div class="line">ngram_set = set()</div><div class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> X_train_padded_seqs:</div><div class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, ngram+<span class="number">1</span>):</div><div class="line">    set_of_ngram = create_ngram(sentence, i)</div><div class="line">    ngram_set.update(set_of_ngram)</div><div class="line"><span class="comment"># 给n-gram词汇编码</span></div><div class="line">start_index = len(vocab) + <span class="number">2</span></div><div class="line">token_indice = &#123;v: k + start_index <span class="keyword">for</span> k, v <span class="keyword">in</span> enumerate(ngram_set)&#125; <span class="comment"># 给n-gram词汇编码</span></div><div class="line">indice_token = &#123;token_indice[k]: k <span class="keyword">for</span> k <span class="keyword">in</span> token_indice&#125;</div><div class="line">max_features = np.max(list(indice_token.keys())) + <span class="number">1</span></div><div class="line"><span class="comment"># 将n-gram词加入到输入文本的末端</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_ngram</span><span class="params">(sequences, token_indice, ngram_range)</span>:</span></div><div class="line">    new_sequences = []</div><div class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> sequences:</div><div class="line">        new_list = sent[:]</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(new_list) - ngram_range + <span class="number">1</span>):</div><div class="line">            <span class="keyword">for</span> ngram_value <span class="keyword">in</span> range(<span class="number">2</span>, ngram_range + <span class="number">1</span>):</div><div class="line">                ngram = tuple(new_list[i:i + ngram_value])</div><div class="line">                <span class="keyword">if</span> ngram <span class="keyword">in</span> token_indice:</div><div class="line">                    new_list.append(token_indice[ngram])</div><div class="line">        new_sequences.append(new_list)</div><div class="line">    <span class="keyword">return</span> new_sequences</div><div class="line">  </div><div class="line">x_train = add_ngram(X_train_word_ids, token_indice, ngram)</div><div class="line">x_test = add_ngram(X_test_word_ids, token_indice, ngram)</div><div class="line">x_train = pad_sequences(x_train, maxlen=<span class="number">25</span>)</div><div class="line">x_test = pad_sequences(x_test, maxlen=<span class="number">25</span>)</div><div class="line"></div><div class="line">model = Sequential()</div><div class="line">model.add(Embedding(max_features, <span class="number">300</span>, input_length=<span class="number">25</span>))</div><div class="line">model.add(GlobalAveragePooling1D())</div><div class="line">model.add(Dense(num_labels, activation=<span class="string">'softmax'</span>))</div></pre></td></tr></table></figure></li><li><p>Char-Level</p><p>我分别测试了TextCNN和RNN模型采用字符作为输入时的模型性能，发现不尽如人意，究其原因，可能是字级别粒度太细，而且文本是短文本，并不能反映出什么有效的信息。前者能达到0.4勉强及格的准确率，而后者只能达到0.28的准确率。因此，不再尝试对短文本进行字粒度的考证。这里想简单应用字符级别的输入，只需对原始文本稍作改变即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">all_sent = [] <span class="comment"># 用于存放新的文本 如 A m y ' s   J i g s a w   S c r a p b o o k</span></div><div class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> title.tolist():</div><div class="line">    new = []</div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sent:</div><div class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> word:</div><div class="line">            new.append(word)</div><div class="line">        new_sent = <span class="string">" "</span>.join(new)</div><div class="line">    all_sent.append(new_sent)</div></pre></td></tr></table></figure></li><li><p>To-do list:</p><p>还有一些论文还没有实现，接下来我会继续更新，敬请期待。。。</p><ul><li>Aspect Level Sentiment</li><li>Dynamic K-max Pooling</li></ul></li><li><p>模型结果比较</p><p>每个模型我都尝试了使用预训练的Glove词向量，效果都不如直接从原文本训练来的好，可能的原因是，文本内容比较简单，而300维的Glove词向量是根据海量语料训练的，看来并不能简单使用Word2Vec和Glove的训练好的向量。需要注意的是，我将最后一个epoch（总共12个epoch）的结果最为模型最终的准确率而且并没有做交叉验证，这样的做法不太合理，可能存在有的模型过拟合了，有的还是欠拟合状态的，简单粗暴吧。每个模型的训练时间和准确率如下图：</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-9-8/55497247.jpg" alt=""></p><p>我们可以看到CNN模型的训练一般都比较快，性能也不算太差。可能是由于语料相对简单，简单的模型如FastText和MLP在准确率上名列前茅，精心设计的模型反而不如简单的模型，有点小题大作的意思。接下来，我们看看更加复杂的推特语料，这些复杂的模型会不会给我们带来一些惊喜呢？</p></li></ul><h2 id="推文情感分析"><a href="#推文情感分析" class="headerlink" title="推文情感分析"></a>推文情感分析</h2><p>之前我提到其实，这个游戏标题分类任务并没有什么实际的意义，只是为了熟悉模型，小试牛刀，接下来我们来试试更大的数据集，比如推文的情感分析任务。标注好的情感分析任务可以当做文本分任务来处理。数据集来自密歇根大学的课程作业<a href="https://inclass.kaggle.com/c/si650winter11" target="_blank" rel="external">SI650 - Sentiment Classification</a> 和由Niek Sanders收集的推文<a href="http://thinknook.com/wp-content/uploads/2012/09/Sentiment-Analysis-Dataset.zip" target="_blank" rel="external">数据集</a>，总共有大约157W条推文，目标变量为0或1，表示消极和积极的情感，是一个大数据集的二分类任务。文中提到使用朴素贝叶斯分类器可以达到75%的准确率，为了验证，我分别使用NB和SVM模型对全部数据集做了测试，NB的准确率在77.5%，SVM为73%。我们来看看深度学习模型是否会有大幅度的提升呢？</p><p>数据读入</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入要使用到的库</span></div><div class="line"><span class="keyword">import</span> csv</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">from</span> nltk.tokenize <span class="keyword">import</span> TweetTokenizer <span class="comment"># a tweet tokenizer from nltk.</span></div><div class="line"></div><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</div><div class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest, chi2</div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegressionCV</div><div class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</div><div class="line"></div><div class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</div><div class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</div><div class="line"><span class="keyword">from</span> keras.utils.np_utils <span class="keyword">import</span> to_categorical</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential, Model</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Embedding, Activation, merge, Input, Lambda, Reshape</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Convolution1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span>  BatchNormalization</div><div class="line"></div><div class="line"><span class="comment"># 出师不利 碰到推文中有逗号，pandas无法解析</span></div><div class="line"><span class="keyword">with</span> open(<span class="string">"tweets.csv"</span>, <span class="string">"r"</span>) <span class="keyword">as</span> infile, open(<span class="string">"quoted.csv"</span>, <span class="string">"wb"</span>) <span class="keyword">as</span> outfile:</div><div class="line">    reader = csv.reader(infile)</div><div class="line">    writer = csv.writer(outfile)</div><div class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> reader:</div><div class="line">        newline = [<span class="string">','</span>.join(line[:<span class="number">-3</span>])] + line[<span class="number">-3</span>:]</div><div class="line">        writer.writerow(newline)</div><div class="line">        </div><div class="line">df = pd.read_csv(<span class="string">'quoted.csv'</span>)</div><div class="line"><span class="comment"># 发现两个没有正确解析的样本，直接忽略好了</span></div><div class="line">df = df.drop(df.index[[<span class="number">8834</span>,<span class="number">535880</span>]])</div><div class="line">df[<span class="string">'Sentiment'</span>] = df[<span class="string">'Sentiment'</span>].map(int)</div><div class="line">df.reset_index(inplace=<span class="keyword">True</span>, drop=<span class="keyword">True</span>)</div><div class="line"><span class="comment"># 为了之后读取方便，建议保存成python特有的pkl格式的文件</span></div><div class="line"><span class="comment"># 去除无效的列</span></div><div class="line">df.drop([<span class="string">'ItemID'</span>, <span class="string">'SentimentSource'</span>], axis=<span class="number">1</span>, inplace=<span class="keyword">True</span>)</div><div class="line">pd.to_pickle(df, <span class="string">'tweet_dataset.pkl'</span>)</div></pre></td></tr></table></figure><p>输入文本处理</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 刚开始想直接应用之前的处理方法，发现渣本由于显存太小，根本跑不起来</span></div><div class="line"><span class="comment"># 于是减少一些词汇量和输入文本的大小</span></div><div class="line"><span class="comment"># 随机抽取10W条推文</span></div><div class="line">df = df.sample(<span class="number">100000</span>)</div><div class="line"><span class="comment"># 先处理目标变量</span></div><div class="line">Y = df.Sentiment.values</div><div class="line">Y = to_categorical(Y)</div><div class="line"><span class="comment"># 分词</span></div><div class="line"><span class="comment"># 另外，nltk包还有一个内置的专门处理推特文本的分词器</span></div><div class="line">tokenizer = Tokenizer(filters=<span class="string">'!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`&#123;|&#125;~\t\n'</span>,lower=<span class="keyword">True</span>,split=<span class="string">" "</span>)</div><div class="line">tokenizer.fit_on_texts(df.SentimentText)</div><div class="line">vocab = tokenizer.word_index</div><div class="line"><span class="comment"># 划分train/test set</span></div><div class="line">x_train, x_test, y_train, y_test = train_test_split(df.SentimentText, Y, test_size=<span class="number">0.2</span>, random_state=<span class="number">2017</span>)</div><div class="line"><span class="comment"># 对文本进行编码(40是所有文本的90%左右的长度，超过的部分直接截去，不足的以0补足)</span></div><div class="line">x_train_word_ids = tokenizer.texts_to_sequences(x_train)</div><div class="line">x_test_word_ids = tokenizer.texts_to_sequences(x_test)</div><div class="line">x_train_padded_seqs = pad_sequences(x_train_word_ids, maxlen=<span class="number">64</span>)</div><div class="line">x_test_padded_seqs = pad_sequences(x_test_word_ids, maxlen=<span class="number">64</span>)</div></pre></td></tr></table></figure><p>跑模型（以TextCNN为例）</p><p>这里我参照的是知乎看山杯比赛<a href="https://zhuanlan.zhihu.com/p/28923961" target="_blank" rel="external">第一名</a>的模型，是TextCNN的扩展版本。他在原先模型的基础上</p><ul><li>使用两层卷积</li></ul><ul><li>使用更多的卷积核，更多尺度的卷积核</li><li>使用了BatchNorm</li><li>分类的时候使用了两层的全连接</li></ul><p>总之就是更深更复杂，所以我也来试试，依样画葫芦。</p><p><img src="https://pic4.zhimg.com/v2-d05248ae98b7e8c759e712796b08b09b_b.png" alt="img"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div></pre></td><td class="code"><pre><div class="line">main_input = Input(shape=(<span class="number">64</span>,), dtype=<span class="string">'float64'</span>)</div><div class="line">embedder = Embedding(len(vocab) + <span class="number">1</span>, <span class="number">256</span>, input_length = <span class="number">64</span>)</div><div class="line">embed = embedder(main_input)</div><div class="line"><span class="comment"># cnn1模块，kernel_size = 3</span></div><div class="line">conv1_1 = Convolution1D(<span class="number">256</span>, <span class="number">3</span>, padding=<span class="string">'same'</span>)(embed)</div><div class="line">bn1_1 = BatchNormalization()(conv1_1)</div><div class="line">relu1_1 = Activation(<span class="string">'relu'</span>)(bn1_1)</div><div class="line">conv1_2 = Convolution1D(<span class="number">128</span>, <span class="number">3</span>, padding=<span class="string">'same'</span>)(relu1_1)</div><div class="line">bn1_2 = BatchNormalization()(conv1_2)</div><div class="line">relu1_2 = Activation(<span class="string">'relu'</span>)(bn1_2)</div><div class="line">cnn1 = MaxPool1D(pool_size=<span class="number">4</span>)(relu1_2)</div><div class="line"><span class="comment"># cnn2模块，kernel_size = 4</span></div><div class="line">conv2_1 = Convolution1D(<span class="number">256</span>, <span class="number">4</span>, padding=<span class="string">'same'</span>)(embed)</div><div class="line">bn2_1 = BatchNormalization()(conv2_1)</div><div class="line">relu2_1 = Activation(<span class="string">'relu'</span>)(bn2_1)</div><div class="line">conv2_2 = Convolution1D(<span class="number">128</span>, <span class="number">4</span>, padding=<span class="string">'same'</span>)(relu2_1)</div><div class="line">bn2_2 = BatchNormalization()(conv2_2)</div><div class="line">relu2_2 = Activation(<span class="string">'relu'</span>)(bn2_2)</div><div class="line">cnn2 = MaxPool1D(pool_size=<span class="number">4</span>)(relu2_2)</div><div class="line"><span class="comment"># cnn3模块，kernel_size = 5</span></div><div class="line">conv3_1 = Convolution1D(<span class="number">256</span>, <span class="number">5</span>, padding=<span class="string">'same'</span>)(embed)</div><div class="line">bn3_1 = BatchNormalization()(conv3_1)</div><div class="line">relu3_1 = Activation(<span class="string">'relu'</span>)(bn3_1)</div><div class="line">conv3_2 = Convolution1D(<span class="number">128</span>, <span class="number">5</span>, padding=<span class="string">'same'</span>)(relu3_1)</div><div class="line">bn3_2 = BatchNormalization()(conv3_2)</div><div class="line">relu3_2 = Activation(<span class="string">'relu'</span>)(bn3_2)</div><div class="line">cnn3 = MaxPool1D(pool_size=<span class="number">4</span>)(relu3_2)</div><div class="line"><span class="comment"># 拼接三个模块</span></div><div class="line">cnn = concatenate([cnn1,cnn2,cnn3], axis=<span class="number">-1</span>)</div><div class="line">flat = Flatten()(cnn)</div><div class="line">drop = Dropout(<span class="number">0.5</span>)(flat)</div><div class="line">fc = Dense(<span class="number">512</span>)(drop)</div><div class="line">bn = BatchNormalization()(fc)</div><div class="line">main_output = Dense(<span class="number">2</span>, activation=<span class="string">'sigmoid'</span>)(bn)</div><div class="line">model = Model(inputs = main_input, outputs = main_output)</div><div class="line"></div><div class="line">model.compile(loss=<span class="string">'binary_crossentropy'</span>,</div><div class="line">              optimizer=<span class="string">'adam'</span>,</div><div class="line">              metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line">history = model.fit(x_train_padded_seqs, y_train,</div><div class="line">                    batch_size=<span class="number">32</span>,</div><div class="line">                    epochs=<span class="number">5</span>,</div><div class="line">                    validation_data=(x_test_padded_seqs, y_test))</div></pre></td></tr></table></figure><p>可视化loss和准确率（更高级的可视化工具是Tensorflow的Tensorboard工具，这里只是简单看下）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</div><div class="line"></div><div class="line">plt.subplot(<span class="number">211</span>)</div><div class="line">plt.title(<span class="string">"Accuracy"</span>)</div><div class="line">plt.plot(history.history[<span class="string">"acc"</span>], color=<span class="string">"g"</span>, label=<span class="string">"Train"</span>)</div><div class="line">plt.plot(history.history[<span class="string">"val_acc"</span>], color=<span class="string">"b"</span>, label=<span class="string">"Test"</span>)</div><div class="line">plt.legend(loc=<span class="string">"best"</span>)</div><div class="line"></div><div class="line">plt.subplot(<span class="number">212</span>)</div><div class="line">plt.title(<span class="string">"Loss"</span>)</div><div class="line">plt.plot(history.history[<span class="string">"loss"</span>], color=<span class="string">"g"</span>, label=<span class="string">"Train"</span>)</div><div class="line">plt.plot(history.history[<span class="string">"val_loss"</span>], color=<span class="string">"b"</span>, label=<span class="string">"Test"</span>)</div><div class="line">plt.legend(loc=<span class="string">"best"</span>)</div><div class="line"></div><div class="line">plt.tight_layout()</div><div class="line">plt.show()</div></pre></td></tr></table></figure><p>但是结果却不尽如人意，出现了严重的过拟合，最终取第二个batch结束后的准确率为76.75%，训练时间为520秒。由于这个模型比之前的模型都要复杂，即使使用了BN技术加速训练，仍旧需要10分钟左右的训练时间。本文使用的GPU为GTX1060(3GB)。什么时候能够拥有一台跑模型的服务器啊？ T T</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-9-9/22887552.jpg" alt=""></p><p>我们使用了复杂的模型和Adam优化器使得模型在训练集上表现出色，准确率不断提升。但是却在测试集上出现了过拟合。过拟合的解决思路一般有以下三点：</p><ul><li>加入正则化技术，如L2正则项，dropout，BatchNormalization等</li><li>更多的训练数据，这里我们只使用了10%不到的数据，就已经超越了朴素贝叶斯使用全部数据集的准确率</li><li>调整超参数，这个是玄学，凭经验吧 - -</li></ul><p>之前由于对几乎没有语义的游戏标题短文本，预训练的Glove词向量，并没有发挥作用。这次，我们再来看看是否有效？</p><p>读入Glove词向量</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 打开词向量文件，每一行的第一个变量是词，后面的一串数字是对应的词向量</span></div><div class="line">GLOVE_DIR = <span class="string">"D:\python\kaggle\game_reviews\glove"</span></div><div class="line">embeddings_index = &#123;&#125;</div><div class="line">f = open(os.path.join(GLOVE_DIR, <span class="string">'glove.6B.200d.txt'</span>), encoding = <span class="string">'utf-8'</span>)</div><div class="line"><span class="keyword">for</span> line <span class="keyword">in</span> f:</div><div class="line">    values = line.split()</div><div class="line">    word = values[<span class="number">0</span>]</div><div class="line">    coefs = np.asarray(values[<span class="number">1</span>:], dtype=<span class="string">'float32'</span>)</div><div class="line">    embeddings_index[word] = coefs</div><div class="line">f.close()</div><div class="line"><span class="comment"># 预训练的词向量中没有出现的词用0向量表示</span></div><div class="line">embedding_matrix = np.zeros((len(vocab) + <span class="number">1</span>, <span class="number">200</span>))</div><div class="line"><span class="keyword">for</span> word, i <span class="keyword">in</span> vocab.items():</div><div class="line">    embedding_vector = embeddings_index.get(word)</div><div class="line">    <span class="keyword">if</span> embedding_vector <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>:</div><div class="line">        embedding_matrix[i] = embedding_vector</div><div class="line"><span class="comment"># 模型中需要修改的仅仅是这里</span></div><div class="line">embedder = Embedding(len(vocab) + <span class="number">1</span>, <span class="number">200</span>, input_length = <span class="number">64</span>, weights = [embedding_matrix], trainable = <span class="keyword">False</span>)</div></pre></td></tr></table></figure><p>神奇的词向量，模型不再严重过拟合了，准确率进一步提升到了77.63%，训练时间缩短了一半！</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-9-9/10302888.jpg" alt=""></p><p>为了证明之前的防过拟合策略是正确的，我们增加一倍的训练集，来看看效果。不出意外，果然提升了！至此，模型基本达到了我们想要的状态，准确进一步提升来到了79%，估计进行完整的调参的话，可以上到80%以上。</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-9-9/82840996.jpg" alt=""></p><h2 id="京东评论"><a href="#京东评论" class="headerlink" title="京东评论"></a>京东评论</h2><p>我们再来试试中文的文本分类。中文天生有个坑需要去跨越，那就是分词，对于一般的文本，现有的分词工具已经能够出色地完成任务了。而面对网络文本，目前还没有看到有效的解决方案，这篇<a href="http://www.aclweb.org/anthology/N16-1030" target="_blank" rel="external">论文</a>尝试使用Bi-LSTM+CRF的方法来实现深度学习模型分词器，有空可以来研究下。所以，本文目前还是使用的主流的分词工具结巴分词。</p><p>根据用户给的评分，5分视作好评，2,3,4分视作中评，1分视作差评。于是，我们便得到了目标类别，共三类。其实，中评的定义很模糊，用户自身也很难去判断，中评的文本中会有一定的好评或者差评的倾向性，对于文本分类造成一定的困难。</p><p>传统机器学习方法（TF-IDF + 朴素贝叶斯/SVM）</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div><div class="line">43</div><div class="line">44</div><div class="line">45</div><div class="line">46</div><div class="line">47</div><div class="line">48</div><div class="line">49</div><div class="line">50</div><div class="line">51</div><div class="line">52</div><div class="line">53</div><div class="line">54</div><div class="line">55</div><div class="line">56</div><div class="line">57</div><div class="line">58</div><div class="line">59</div></pre></td><td class="code"><pre><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</div><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</div><div class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</div><div class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</div><div class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</div><div class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> metrics</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</div><div class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</div><div class="line"><span class="keyword">from</span> data_helper_ml <span class="keyword">import</span> load_data_and_labels</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line">categories = [<span class="string">'good'</span>, <span class="string">'bad'</span>, <span class="string">'mid'</span>]</div><div class="line"><span class="comment"># 我在data_helper_ml文件中定义了一些文本清理任务，如输入文本处理，去除停用词等</span></div><div class="line">x_text, y = load_data_and_labels(<span class="string">"./data/good_cut_jieba.txt"</span>, <span class="string">"./data/bad_cut_jieba.txt"</span>, <span class="string">"./data/mid_cut_jieba.txt"</span>)</div><div class="line"><span class="comment"># 划分数据集</span></div><div class="line">x_train, x_test, y_train, y_test = train_test_split(x_text, y, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</div><div class="line">y = y.ravel()</div><div class="line">y_train = y_train.ravel()</div><div class="line">y_test = y_test.ravel()</div><div class="line"></div><div class="line">print(<span class="string">"Train/Test split: &#123;:d&#125;/&#123;:d&#125;"</span>.format(len(y_train), len(y_test)))</div><div class="line"></div><div class="line"><span class="string">""" Naive Bayes classifier """</span></div><div class="line"><span class="comment"># sklearn有一套很成熟的管道流程Pipeline，快速搭建机器学习模型神器</span></div><div class="line">bayes_clf = Pipeline([(<span class="string">'vect'</span>, CountVectorizer()), </div><div class="line">                      (<span class="string">'tfidf'</span>, TfidfTransformer()),</div><div class="line">                      (<span class="string">'clf'</span>, MultinomialNB()) </div><div class="line">                      ])</div><div class="line">bayes_clf.fit(x_train, y_train)</div><div class="line"><span class="string">""" Predict the test dataset using Naive Bayes"""</span></div><div class="line">predicted = bayes_clf.predict(x_test)</div><div class="line">print(<span class="string">'Naive Bayes correct prediction: &#123;:4.4f&#125;'</span>.format(np.mean(predicted == y_test)))</div><div class="line"><span class="comment"># 输出f1分数，准确率，召回率等指标</span></div><div class="line">print(metrics.classification_report(y_test, predicted, target_names=categories))</div><div class="line"></div><div class="line"><span class="string">""" Support Vector Machine (SVM) classifier"""</span></div><div class="line">svm_clf = Pipeline([(<span class="string">'vect'</span>, CountVectorizer()),</div><div class="line">    (<span class="string">'tfidf'</span>, TfidfTransformer()),</div><div class="line">    (<span class="string">'clf'</span>, SGDClassifier(loss=<span class="string">'hinge'</span>, penalty=<span class="string">'l2'</span>, alpha=<span class="number">1e-3</span>, max_iter= <span class="number">5</span>, random_state=<span class="number">42</span>)),</div><div class="line">])</div><div class="line">svm_clf.fit(x_train, y_train)</div><div class="line">predicted = svm_clf.predict(x_test)</div><div class="line">print(<span class="string">'SVM correct prediction: &#123;:4.4f&#125;'</span>.format(np.mean(predicted == y_test)))</div><div class="line">print(metrics.classification_report(y_test, predicted, target_names=categories))</div><div class="line"><span class="comment"># 输出混淆矩阵</span></div><div class="line">print(<span class="string">"Confusion Matrix:"</span>)</div><div class="line">print(metrics.confusion_matrix(y_test, predicted))</div><div class="line">print(<span class="string">'\n'</span>)</div><div class="line"></div><div class="line"><span class="string">""" 10-折交叉验证 """</span></div><div class="line">clf_b = make_pipeline(CountVectorizer(), TfidfTransformer(), MultinomialNB())</div><div class="line">clf_s= make_pipeline(CountVectorizer(), TfidfTransformer(), SGDClassifier(loss=<span class="string">'hinge'</span>, penalty=<span class="string">'l2'</span>, alpha=<span class="number">1e-3</span>, n_iter= <span class="number">5</span>, random_state=<span class="number">42</span>))</div><div class="line"></div><div class="line">bayes_10_fold = cross_val_score(clf_b, x_text, y, cv=<span class="number">10</span>)</div><div class="line">svm_10_fold = cross_val_score(clf_s, x_text, y, cv=<span class="number">10</span>)</div><div class="line"></div><div class="line">print(<span class="string">'Naives Bayes 10-fold correct prediction: &#123;:4.4f&#125;'</span>.format(np.mean(bayes_10_fold)))</div><div class="line">print(<span class="string">'SVM 10-fold correct prediction: &#123;:4.4f&#125;'</span>.format(np.mean(svm_10_fold)))</div></pre></td></tr></table></figure><p>结果如下图：</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-9-9/33303016.jpg" alt=""></p><p>深度学习模型</p><figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 读取经过jiaba分词后的三个数据文件</span></div><div class="line">good_examples = list(open(good_data_file, <span class="string">"r"</span>, encoding=<span class="string">'utf-8'</span>).readlines())</div><div class="line">bad_examples = list(open(bad_data_file, <span class="string">"r"</span>, encoding=<span class="string">'utf-8'</span>).readlines())</div><div class="line">mid_examples = list(open(mid_data_file, <span class="string">"r"</span>, encoding=<span class="string">'utf-8'</span>).readlines())</div><div class="line"><span class="comment"># 清理一些无效字符，clean为自定义函数</span></div><div class="line">good_examples = [clean(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> good_examples]</div><div class="line">bad_examples = [clean(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> bad_examples]</div><div class="line">mid_examples = [clean(sent) <span class="keyword">for</span> sent <span class="keyword">in</span> mid_examples]</div><div class="line"><span class="comment"># 删除前后空格</span></div><div class="line">good_examples = [i.strip() <span class="keyword">for</span> i <span class="keyword">in</span> good_examples]</div><div class="line">bad_examples = [i.strip() <span class="keyword">for</span> i <span class="keyword">in</span> bad_examples]</div><div class="line">mid_examples = [i.strip() <span class="keyword">for</span> i <span class="keyword">in</span> mid_examples]</div><div class="line"><span class="comment"># 去除清理过后的空文本， sent_filter为自定义函数</span></div><div class="line">good_examples = sent_filter(good_examples)</div><div class="line">bad_examples = sent_filter(bad_examples)</div><div class="line">mid_examples = sent_filter(mid_examples)</div><div class="line">X = good_examples + bad_examples + mid_examples</div><div class="line"><span class="comment"># 处理目标变量</span></div><div class="line">good_labels = [[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> good_examples]</div><div class="line">bad_labels = [[<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> bad_examples]</div><div class="line">mid_labels = [[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>] <span class="keyword">for</span> _ <span class="keyword">in</span> mid_examples]</div><div class="line">Y = np.concatenate([good_labels, bad_labels, mid_labels], <span class="number">0</span>)</div></pre></td></tr></table></figure><p>为了照顾类别的平衡，在爬取数据的时候尽可能保持各个类别数量的相等，每个类别大约5200条评论左右。之后的步骤就很类似了，由于目前没有很好的中文预训练好的词向量文件，我们这里直接从原文本训练词向量。</p><p>这里我们依旧仿照知乎看山杯的冠军作者来实现一个新的模型架构Inception，Inception的结构源于2015年ImageNet竞赛中的冠军模型的缩小版。模型和之前的TextCNNBN版本很相似，如下图所示：</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-9-9/30969236.jpg" alt=""></p><p>大家也可以自己动手来试试搭建这样一个Inception模型，代码我就不贴出来了:P</p><p>始终没有解决过拟合的问题，可能是数据问题，模型之间的差距并不明显，最终结果如下：</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-9-9/26135016.jpg" alt=""></p><h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>我们来回顾下，我们至此学到了哪些内容。</p><ul><li>文本分类任务的处理流程（文本清理编码-数据集的划分-模型搭建-模型运行评估）</li><li>传统机器学习模型的处理方法（Baseline：TF-IDF + 分类器）</li><li>常见的深度学习模型（CNN，RNN，Attention）</li><li>历年经典论文中的模型复现（TextCNN，RCNN，HAN等）</li><li>实战（多分类的小数据集，推文情感分析，京东评论好中差评）</li></ul><p>最后给大家推荐一些高质量的学习资源：</p><ul><li>NLP大牛 Chris Manning 和 Richard Socher 在 Stanford 合开的课程CS224d【<a href="http://cs224d.stanford.edu/syllabus.html" target="_blank" rel="external">课程链接</a>】（一直没有静下心来认真学习这门课程 T T）</li><li>Google Brain成员Christopher Olah大神的理论<a href="http://colah.github.io/" target="_blank" rel="external">博客</a></li><li>另一位Google Brain成员Denny Britz大神关于NLP的博客 <a href="http://www.wildml.com/" target="_blank" rel="external">WILDML</a></li><li>MOOC课程资源：优达学城深度学习基石纳米<a href="https://cn.udacity.com/dlnd" target="_blank" rel="external">学位</a>和吴恩达三大项目之一<a href="https://www.deeplearning.ai/" target="_blank" rel="external">deeplearning.ai</a></li><li>方得智能大神brightmart的Github <a href="https://github.com/brightmart/text_classification" target="_blank" rel="external">Repo</a></li><li>最近亚马逊的DL大牛李沐老师于近期也推出了基于MXNet的Gluon框架的系列<a href="http://zh.gluon.ai/" target="_blank" rel="external">课程</a></li></ul><p>希望大家能够有所收获，第一次自己动手码字，有很多论文和模型的理解还不够深刻！关于代码部分，我接下来会整理好放到我的<a href="https://github.com/stevewyl/keras_text_classification" target="_blank" rel="external">github</a>上。欢迎大家向我提出宝贵的意见！最后祝大家玩的开心！:P</p><p>如果你喜欢我的文章，欢迎转发，注明转发来源即可，链接二维码在此 :D</p><p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-9-9/59375050.jpg" alt=""></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;写在前面&quot;&gt;&lt;a href=&quot;#写在前面&quot; class=&quot;headerlink&quot; title=&quot;写在前面&quot;&gt;&lt;/a&gt;写在前面&lt;/h2&gt;&lt;p&gt;从优达DLND毕业后，一直想自己动手做点什么来着，互助班的导师也鼓励自己动手写点心得体验啥的。之前一直没怎么观看Youtub
      
    
    </summary>
    
      <category term="深度学习" scheme="https://stevewyl.github.io/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/"/>
    
    
      <category term="文本分类" scheme="https://stevewyl.github.io/tags/%E6%96%87%E6%9C%AC%E5%88%86%E7%B1%BB/"/>
    
      <category term="Keras" scheme="https://stevewyl.github.io/tags/Keras/"/>
    
      <category term="CNN" scheme="https://stevewyl.github.io/tags/CNN/"/>
    
      <category term="RNN" scheme="https://stevewyl.github.io/tags/RNN/"/>
    
  </entry>
  
</feed>
