<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hexo, NexT" />








  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=5.1.2" />






<meta property="og:type" content="website">
<meta property="og:title" content="DL炼丹房">
<meta property="og:url" content="https://stevewyl.github.io/index.html">
<meta property="og:site_name" content="DL炼丹房">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="DL炼丹房">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://stevewyl.github.io/"/>





  <title>DL炼丹房</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">DL炼丹房</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://stevewyl.github.io/2017/08/27/keras/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yilei Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/dabai.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DL炼丹房">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2017/08/27/keras/" itemprop="url">Keras之文本分类实现</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-27T19:41:01+08:00">
                2017-08-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          
             <span id="/2017/08/27/keras/" class="leancloud_visitors" data-flag-title="Keras之文本分类实现">
               <span class="post-meta-divider">|</span>
               <span class="post-meta-item-icon">
                 <i class="fa fa-eye"></i>
               </span>
               
                 <span class="post-meta-item-text">阅读次数&#58;</span>
               
                 <span class="leancloud-visitors-count"></span>
             </span>
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>从优达DLND毕业后，一直想自己动手做点什么来着，互助班的导师也鼓励自己动手写点心得体验啥的。之前一直没怎么观看Youtube网红Siraj老师的课程视频，他每个视频最后都会有一个编程挑战。于是，想着先从自己熟悉的内容着手吧，Siraj老师第三周的编程挑战是做一个多类别的文本分类器，链接在此：<a href="https://github.com/llSourcell/How_to_do_Sentiment_Analysis" target="_blank" rel="external">Github</a>，那就来试试吧。除了想自己练练手外，也顺便把模型都好好梳理一遍。为了给自己增加些难度，是否有可能把过去几年内那些大牛们论文中的模型复现出来呢？阅读这篇文章，需要你对自然语言处理和深度学习的模型有一个基础的了解哦！</p>
<h2 id="文本多分类"><a href="#文本多分类" class="headerlink" title="文本多分类"></a>文本多分类</h2><p>首先我们来看下数据集长什么样子吧 :P Let‘s get started!</p>
<p>我们使用pandas来加载数据，数据集来自IGN.com，收集了过去20年各大游戏厂商发布的游戏数据，如发布日期，发布平台，游戏评价等变量，这里有一篇关于这个数据集很不错的分析教程 <a href="https://www.kaggle.com/ash316/20-years-of-games-analysis" target="_blank" rel="external">Kaggle</a> 。而我们现在想分析下游戏名与用户评价之间的关系，看上去并不合理，我们姑且按照Siraj老师的任务来试试。于是，游戏名作为文本变量将作为模型的输入X，而用户评价词作为文本类别Y。<br>然后来看看各个类别的数量，为了避免类别样本数的不平衡，我们这里把关于评价为Disaster的游戏去除。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">df = pd.read_csv(<span class="string">'ign.csv'</span>).iloc[:, <span class="number">1</span>:<span class="number">3</span>]</div><div class="line">df.score_phrase.value_counts()</div><div class="line">df = df[df.score_phrase != <span class="string">'Disaster'</span>]</div></pre></td></tr></table></figure>
<p>首先我们先来试试传统机器学习模型对文本分类任务常见的做法吧</p>
<h3 id="传统文本分类方法"><a href="#传统文本分类方法" class="headerlink" title="传统文本分类方法"></a>传统文本分类方法</h3><ul>
<li><p>词袋模型</p>
<p>由于计算机只能处理数字型的变量，并不能直接把文本丢给计算机然后让它告诉我们这段文字的类别。于是，我们需要对词进行one-hot编码。假设我们总共有N个词，然后对词进行索引编码并构造一个N维零向量，如果这个文本中的某些词出现，就在该词索引值位置标记为1，表示这个文本包含这个词。于是，我们会得到如下类似的向量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">( 0, 0, 1, 0, .... , 1, ... 0, 0, 1, 0)</div></pre></td></tr></table></figure>
<p>但是，一般来说词库量至少都是百万级别，因此词袋模型有个两个最大的问题：高维度、高稀疏性。这种表示方法还存在一个重要的问题就是”词汇鸿沟”现象：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否有关系。</p>
</li>
<li><p>共现矩阵</p>
<p>为了使用上下文来表示单词间的关系，也有人提出使用基于窗口大小的共现矩阵，但仍然存在数据维度大稀疏的问题。</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/57232353.jpg" alt=""></p>
</li>
<li><p>TF-IDF</p>
<p>TF-IDF 用以评估一字词对于一个文档集或一个语料库中的其中一份文档的重要程度，是一种计算特征权重的方法。核心思想即，字词的重要性随着它在文档中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。有效地规避了那些高频却包含很少信息量的词。我们这里也是用TF-IDF 对文本变量进行特征提取。</p>
</li>
<li><p>分类器</p>
<p>分类器就是常见的机器学习分类模型了，常用的有以下两种，这里我不再赘述这两个模型的原理了。</p>
<ul>
<li>朴素贝叶斯：从垃圾邮件识别应用开始被广泛使用</li>
<li>支持向量机：这篇<a href="http://www.linkedin.com/pulse/please-explain-support-vector-machines-svm-like-i-am-5-joni-salminen" target="_blank" rel="external">文章</a> 很通俗地解释了SVM的工作原理 </li>
</ul>
</li>
</ul>
<p>使用Scikit-Learn库能够傻瓜似的来实现你的机器学习模型，我们这里使用TfidfVectorizer函数对文本进行特征处理，并去除停用词，模型有多类别朴素贝叶斯和线性SVM分类器。结果很不令人满意，NB模型结果稍好，准确率为28%，领先SVM 1%。下面我们来看看深度学习模型强大的性能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">vect = TfidfVectorizer(stop_words=<span class="string">'english'</span>, token_pattern=<span class="string">r'\b\w&#123;2,&#125;\b'</span>,</div><div class="line">                       min_df=<span class="number">1</span>, max_df=<span class="number">0.1</span>, ngram_range=(<span class="number">1</span>,<span class="number">2</span>))</div><div class="line">mnb = MultinomialNB(alpha=<span class="number">2</span>)</div><div class="line">svm = SGDClassifier(loss=<span class="string">'hinge'</span>, penalty=<span class="string">'l2'</span>, alpha=<span class="number">1e-3</span>, n_iter=<span class="number">5</span>, random_state=<span class="number">42</span>)</div><div class="line">mnb_pipeline = make_pipeline(vect, mnb)</div><div class="line">svm_pipeline = make_pipeline(vect, svm)</div><div class="line">mnb_cv = cross_val_score(mnb_pipeline, title, label, scoring=<span class="string">'accuracy'</span>, cv=<span class="number">10</span>, n_jobs=<span class="number">-1</span>)</div><div class="line">svm_cv = cross_val_score(svm_pipeline, title, label, scoring=<span class="string">'accuracy'</span>, cv=<span class="number">10</span>, n_jobs=<span class="number">-1</span>)</div><div class="line">print(<span class="string">'\nMultinomialNB Classifier\'s Accuracy: %0.5f\n'</span> % mnb_cv.mean())</div><div class="line">print(<span class="string">'\nSVM Classifier\'s Accuracy: %0.5f\n'</span> % svm_cv.mean())</div></pre></td></tr></table></figure>
<h3 id="走进NLP和DL"><a href="#走进NLP和DL" class="headerlink" title="走进NLP和DL"></a>走进NLP和DL</h3><p>传统方法对于文本的特征表达能力很弱，神经网络同样不擅长处理这样高维度高稀疏性的数据，因此我们需要对文本做进一步的特征处理，这里就要讲到词嵌入的概念了。深度学习模型中，一个单词常常用一个低维且稠密的向量来表示，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">( 0.286, 0.792, -0.177, -0.107, .... , 0.109, ... 0.349, 0.271, -0.642)</div></pre></td></tr></table></figure>
<ul>
<li><p>词向量</p>
<p>主流的词嵌入实现方法有Mikolov的<a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="external">Word2Vec</a>和斯坦福大学的<a href="https://nlp.stanford.edu/pubs/glove.pdf" target="_blank" rel="external">Glove</a>，也有人做过<a href="https://rare-technologies.com/making-sense-of-word2vec/" target="_blank" rel="external">实验</a>来比较这两种方法的优劣，并没有多大的差异。Word2Vec是基于预测的词向量模型，简单来说，就是给定一个词，去预测这个词周围可能出现的词，或者给定一些词来确定位于中心位置的词。而Glove是基于统计方法的，通过对词-词共现矩阵里的非零元素进行训练。总体来说，Word2Vec使用的是局部信息，而Glove使用的是全局信息，因此前者训练起来更慢但不占用内存，而Glove通过更多的计算资源换取训练速度上的提升。具体的实现细节可以参考这两篇论文。本文中将使用预训练的Glove300维词向量和由自己文本生成的词向量。</p>
</li>
</ul>
<p>为了快速实现模型，这篇文章中将使用Keras（TensorFlow的高级API）来完成。要使用Keras前，你必须安装TensorFlow作为其后端，Keras目前支持Tensorflow、Theano和CNTK作为后端，不过我还是推荐大家使用TensorFlow。Keras的中文<a href="http://keras-cn.readthedocs.io/en/latest/for_beginners/keras_linux/" target="_blank" rel="external">文档</a>能够帮助大家无坑完成安装过程。</p>
<p>我们先来了解一些基础的深度学习模型吧！</p>
<ul>
<li><p>CNN </p>
<p>深度学习入门必学的两大模型之一卷积神经网络。首先我们来理解下什么是卷积操作？卷积，你可以把它想象成一个应用在矩阵上的滑动窗口函数。下图中左边的矩阵表示的是一张黑白图像。每个方格代表了一个像素，0表示黑色，1表示白色。这个滑动窗口称作kernel或者filter。这里我们使用的是一个3*3的filter，将它的值和与其对应的原图像矩阵进行相乘，然后再将它们相加。这样我们在整个原图矩阵上滑动filter来遍历所有像素后得到一个完整的卷积特征。</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/19849125.jpg" alt=""></p>
<p>卷积网络也就是对输入样本进行多次卷积操作，提取数据中的局部位置的特征，然后再拼接池化层（图中的Pooling层）做进一步的降维操作，最后与全连接层拼接完成对输入样本的全新的特征构造，将新的特征向量输送给分类器（以图片分类为例）进行预测分类。我们可以把CNN类比N-gram模型，N-gram也是基于词窗范围这种局部的方式对文本进行特征提取，与CNN的做法很类似，在下文中，我们再来看看如何运用CNN对文本数据进行建模。</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/73075953.jpg" alt=""></p>
<p>卷积网络开始崭露头角是在CV领域，2012年的ImageNet竞赛中，大大降低了图片分类的错误率。为什么CNN在计算机视觉领域这么厉害呢？直观的感受就是：</p>
<ul>
<li>它能够学习识别基本的直线，曲线，然后是形状，点块，然后是图片中更复杂的物体。最终CNN分类器把这些大的，复杂的物体综合起来识别图片</li>
<li>在下图中的例子中，可以看作这样的层级关系：<ul>
<li>简单的形状，如椭圆，暗色圆圈</li>
<li>复杂的物体（简单形状的组合），例如眼睛，鼻子，毛发</li>
<li>狗的整体（复杂物体的组合）</li>
</ul>
</li>
</ul>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/30339409.jpg" alt=""></p>
</li>
<li><p>RNN</p>
<p>而循环网络与CNN不同的是，CNN学习空间位置上局部位置的特征表示，而RNN学习的是时间顺序上的特征，用来处理序列数据，如股价，文本等。RNN之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点是相互连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。</p>
<p>就像我们说话一样，我们不能把说的话倒过来表示，这样会变得毫无意义，并不会明白你在说什么，也就是说文本中的每个词是包含顺序信息的，由此可以使用RNN对文本数据进行建模。<br><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/18106498.jpg" alt=""></p>
<p>但是，随着时间的不断增加，你的隐藏层一次又一次地乘以权重W。假如某个权重w是一个接近于0或者大于1的数，随着乘法次数的增加，这个权重值会变得很小或者很大，造成反向传播时梯度计算变得很困难，造成梯度爆炸或者梯度消失的情况，模型难以训练。也就是说一般的RNN模型对于长时间距离的信息记忆很差，比如人老了会忘记某件很久发生的事情一样，于是，LSTM和GRU 应运而生。LSTM与GRU很相似，以LSTM为例。</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/54396153.jpg" alt=""></p>
<p>LSTM又称为长短期记忆网络，LSTM 单元乍看起来很复杂。关键的新增部分就在于标记为 C 的单元状态。在这个单元中，有四个显示为黄色框的网络层，每个层都有自己的权重，如以 σ 标记的层是 sigmoid 层。这些红圈表示逐点或逐元素操作。单元状态在通过 LSTM 单元时几乎没有交互，使得大部分信息得以保留，单元状态仅通过这些控制门（gate）进行修改。第一个控制门是遗忘门，用来决定我们会从单元状态中丢弃什么信息。第二个门是更新们，用以确定什么样的新信息被存放到单元状态中。最后一个门是输出门，我们需要确定输出什么样的值。总结来说 LSTM 单元由单元状态和一堆用于更新信息的控制门组成，让信息部分传递到隐藏层状态。更直观的来讲，把LSTM看作是一部电影，可以把单元状态看作是剧情主线，而随着剧情的发展，有些不必要的事件会被遗忘，而一些更加影响主线的剧情会被加入到单元状态中来，不断更新剧情然后输出新的剧情发展。</p>
</li>
<li><p>Attention机制</p>
<p>基于Attention的模型在NLP领域首先被应用于自然语言生成问题中，用于改进机器翻译任务的性能。我们这里也以机器翻译为例来解释下注意力机制的原理。我们可以把翻译任务是一个序列向另一个序列转换的过程。</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-29/69730967.jpg" alt=""></p>
<p>上图就是Seq2Seq模型的基本结构，由编码器（Encoder）和解码器（Decoder）组成。编码器负责将输入的单词按顺序进行信息提取，在最后一步生成的隐藏状态即固定长度的句子的特征向量。然后解码器从这个句子向量中获取信息对文本进行翻译。由于解码器的主要信息来源就是最后一步的隐藏状态，这个h3向量必须尽可能地包含句子的所有必要的信息。这个向量说白了就是句子嵌入（类比词嵌入）。假如我们需要翻译的文本不是很长，这个模型已经能达到很不错的性能。假如我们现在要翻译一句超过50个单词的句子，似乎这个模型很难再hold住，即使你在训练的时候使用了LSTM去提取句子特征，去尽可能保留过去的记忆，但还是达不到想要的结果。</p>
<p>而注意力机制恰恰是为了解决长距离依赖的问题，我们不再需要固定长度的句子向量，而是让解码器自己去输入文本中寻找想要关注的被翻译文本。比如把”I am learning deep learning model“成中文时，我们让解码器去与输入文本中的词对齐，翻译deep的时候去关注deep这个词，而不是平等对待每个有可能的词，找到与输入文本相对应的相同语义的词，而不再是对句子进行特征提取。</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-29/74136352.jpg" alt=""></p>
<p>上图我们可以看到，解码器在翻译下一个词时，需要依赖之前已经翻译好的文本和与输入文本相对齐的那个词。简单描述的话，用解码器t时刻的隐藏状态去和输入文本中的每个单词对应的隐藏状态去比对，通过某个函数f去计算带翻译的单词yi与每个输入单词对齐的可能性。而编码器由Bi-LSTM模型组成。不同的语言的f函数可能会有差别，就像中文和英文，语法结构差异很大，很难按顺序单词一一对齐。由此可以得出结论，注意力机制的核心思想是在翻译每个目标词（或对文本进行分类时）所用的上下文是不同的，这样的考虑显然是更合理的。具体实现请见这篇<a href="https://arxiv.org/pdf/1409.0473v7.pdf" target="_blank" rel="external">论文</a>。而如何将注意力机制运用到文本分类中来，下文会介绍。</p>
</li>
</ul>
<h3 id="深度学习文本分类模型"><a href="#深度学习文本分类模型" class="headerlink" title="深度学习文本分类模型"></a>深度学习文本分类模型</h3><ul>
<li><p>TextCNN</p>
<p>这是CNN首次被应用于文本分类任务的开山之作，可以说，之后很多论文都是基于此进行拓展的。它是由<a href="https://arxiv.org/abs/1408.5882" target="_blank" rel="external">Yoon Kim</a>于2014年提出的，你可以在github上找到各种不同深度学习框架对于这个模型的实现。下面我们来细细品读这篇论文吧。</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-29/75206008.jpg" alt=""></p>
<p>上图很好地诠释了模型的框架。假设我们有一句句子需要对其进行分类。句子中每个词是由n维词向量组成的，也就是说输入矩阵大小为m*n，其中m为句子长度。CNN需要对输入样本进行卷积操作，对于文本数据，filter不再横向滑动，仅仅是向下移动，有点类似于N-gram在提取词与词间的局部相关性。图中共有三种步长策略，分别是2,3,4，每个步长都有两个filter（实际训练时filter数量会很多）。在不同词窗上应用不同filter，最终得到6个卷积后的向量。然后对每一个向量进行最大化池化操作并拼接各个池化值，最终得到这个句子的特征表示，将这个句子向量丢给分类器进行分类，至此完成整个流程。</p>
<p>文中作者还提出了动态的词向量，即将词向量也作为权重变量进行训练，而我们平时常用的产生词向量的方法有从当前数据集中自己产生的词向量和使用预训练好的word2vec或glove词向量，都属于静态词向量范畴，即它们不再网络训练时发生变化。文中实验表明，动态的词向量表现更好。我也将在下一篇博文中尝使用Tensorflow来实现这种动态词向量。</p>
</li>
<li><p>TextCNN with K-pooling</p>
</li>
<li><p>DCNN</p>
</li>
<li><p>CharCNN</p>
</li>
<li><p>CharRNN</p>
<p>​</p>
</li>
<li><p>Bi-LSTM</p>
<p>本篇<a href="https://www.ijcai.org/Proceedings/16/Papers/408.pdf" target="_blank" rel="external">论文</a>由复旦大学的邱锡鹏教授的团队于2015年发表，文中详细地阐述了RNN模型用于文本分类任务的各种变体模型。</p>
</li>
<li><p>CLSTM </p>
</li>
<li><p>RCNN</p>
</li>
<li><p>FastText</p>
<p>FastText是Facebook于2016年发表的论文中提出的一种简单快速实现的 文本分类模型。可能你已经被前面那些复杂的模型搞得七荤八素了，那么这个模型你很快地理解，令人意外的是，它的性能并不差。输入变量是经过embedding的词向量，这里的隐藏层只是一个简单的平均池化层，然后把这个池化过的向量丢给softmax分类器就完成了。另外，这里的X并不仅仅是单个单词，也可以加入N-gram组合的词作为输入的一部分，文中将2-元和3-元的特征也加入到了模型中。本文的思想在于通过简单的特征线性组合就可以达到不错的分类性能，我们可以把fasttext当作是工业界一种快速实现模型的产物。</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-29/18150827.jpg" alt=""></p>
</li>
<li><p>HAN</p>
</li>
<li><p>Attention Model</p>
<ul>
<li>本文只涉及单句，因此构不成文档，只需要 word-level 这一层的注意力即可。加入Attention之后最大的好处自然是能够直观的解释各个句子和词对分类类别的重要性。</li>
</ul>
</li>
</ul>
<h3 id="实战！"><a href="#实战！" class="headerlink" title="实战！"></a>实战！</h3><p>首先我们先要对文本数据进行编码，因为模型只能接受数值型的数据。常见的编码之前有提到过有One-Hot和词嵌入。第一步，来划分训练样本和测试样本。如果需要将你的模型部署到产品中去，则需要更加复杂的划分，详情课见吴恩达最新的AI<a href="https://www.deeplearning.ai/" target="_blank" rel="external">课程</a>中提及的机器学习项目中必须注意的一些问题。这里，我们不需要考虑太多的细节，就简单划分训练集和测试集即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div><div class="line">38</div><div class="line">39</div><div class="line">40</div><div class="line">41</div><div class="line">42</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 导入使用到的库</span></div><div class="line"><span class="keyword">from</span> keras.preprocessing.sequence <span class="keyword">import</span> pad_sequences</div><div class="line"><span class="keyword">from</span> keras.preprocessing.text <span class="keyword">import</span> Tokenizer</div><div class="line"><span class="keyword">from</span> keras.layers.merge <span class="keyword">import</span> concatenate</div><div class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential, Model</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense, Embedding, Activation, Merge, Input, Lambda, Reshape</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Convolution1D, Flatten, Dropout, MaxPool1D, GlobalAveragePooling1D</div><div class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> LSTM, GRU, TimeDistributed, Bidirectional</div><div class="line"><span class="keyword">from</span> keras.utils.np_utils <span class="keyword">import</span> to_categorical</div><div class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> MultinomialNB</div><div class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</div><div class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</div><div class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</div><div class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</div><div class="line"></div><div class="line"><span class="comment"># 划分训练/测试集</span></div><div class="line">X_train, X_test, y_train, y_test = train_test_split(title, label, test_size=<span class="number">0.1</span>, random_state=<span class="number">42</span>)</div><div class="line"></div><div class="line"><span class="comment"># 对类别变量进行编码，共10类</span></div><div class="line">y_labels = list(y_train.value_counts().index)</div><div class="line">le = preprocessing.LabelEncoder()</div><div class="line">le.fit(y_labels)</div><div class="line">num_labels = len(y_labels)</div><div class="line">y_train = to_categorical(y_train.map(<span class="keyword">lambda</span> x: le.transform([x])[<span class="number">0</span>]), num_labels)</div><div class="line">y_test = to_categorical(y_test.map(<span class="keyword">lambda</span> x: le.transform([x])[<span class="number">0</span>]), num_labels)</div><div class="line"></div><div class="line"><span class="comment"># 分词，构建单词-id词典       </span></div><div class="line">tokenizer = Tokenizer(filters=<span class="string">'!"#$%&amp;()*+,-./:;&lt;=&gt;?@[\\]^_`&#123;|&#125;~\t\n'</span>,lower=<span class="keyword">True</span>,split=<span class="string">" "</span>)</div><div class="line">tokenizer.fit_on_texts(title)</div><div class="line">vocab = tokenizer.word_index</div><div class="line"></div><div class="line"><span class="comment"># 将每个词用词典中的数值代替</span></div><div class="line">X_train_word_ids = tokenizer.texts_to_sequences(X_train)</div><div class="line">X_test_word_ids = tokenizer.texts_to_sequences(X_test)</div><div class="line"></div><div class="line"><span class="comment"># One-hot</span></div><div class="line">x_train = tokenizer.sequences_to_matrix(X_train_word_ids, mode=<span class="string">'binary'</span>)</div><div class="line">x_test = tokenizer.sequences_to_matrix(X_test_word_ids, mode=<span class="string">'binary'</span>)</div><div class="line"></div><div class="line"><span class="comment"># 序列模式</span></div><div class="line">x_train = pad_sequences(X_train_word_ids, maxlen=<span class="number">20</span>)</div><div class="line">x_test = pad_sequences(X_test_word_ids, maxlen=<span class="number">20</span>)</div></pre></td></tr></table></figure>
<p>Keras提供两大类模型框架。第一种是Sequential模式，就像搭积木一样，将你想要的网络层拼接起来，可以理解为串联。而另外一种是Model模式，需要你指定模型的输入和输出格式，更加灵活地组合你的网络层，可以理解为串联加并联。搭建完模型结构后，你需要对模型进行编译，这一步你需要指定模型的损失函数，本文是文本多分类任务，所以损失函数是多类别的交叉熵函数。另外， 需要确定损失函数的优化算法和模型评估指标。Adam优化器是目前公认的各项任务中性能最优的，所以本文将全部使用Adam作为优化器。这里有一篇实战<a href="https://blog.slinuxer.com/2016/09/sgd-comparison" target="_blank" rel="external">教程</a>对不同优化器做了性能评估。接着就是模型的训练，使用fit函数，这里需要指定的参数有输入数据，批量大小，迭代轮数，验证数据集等。然后，你就能看到你的模型开始愉快地运行起来了。Keras作为Tensforflow的高级API，对很多细节进行了封装，可以让深度学习小白快速上手，如果你需要实现更加复杂的模型的话，就需要去好好研究下Tensorflow了。下一篇博文目标是用Tensorflow来实现简单的机器翻译任务，也为接下来准备参加的AI Challenger比赛做准备！</p>
<ul>
<li><p>One-Hot + MLP</p>
<p>结果居然出奇的好！卖个关子，最后看看各个模型的最终准确率排名 = =</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div></pre></td><td class="code"><pre><div class="line">model = Sequential()</div><div class="line"><span class="comment"># 全连接层</span></div><div class="line">model.add(Dense(<span class="number">512</span>, input_shape=(len(vocab)+<span class="number">1</span>,), activation=<span class="string">'relu'</span>))</div><div class="line"><span class="comment"># DropOut层</span></div><div class="line">model.add(Dropout(<span class="number">0.5</span>))</div><div class="line"><span class="comment"># 全连接层+分类器</span></div><div class="line">model.add(Dense(num_labels,activation=<span class="string">'softmax'</span>))</div><div class="line"></div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</div><div class="line">              optimizer=<span class="string">'adam'</span>,</div><div class="line">              metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line">model.fit(x_train, y_train,</div><div class="line">          batch_size=<span class="number">32</span>,</div><div class="line">          epochs=<span class="number">15</span>,</div><div class="line">          validation_data=(x_test, y_test))</div></pre></td></tr></table></figure>
</li>
<li><p>CNN</p>
<ul>
<li><p>模仿LeNet-5</p>
<p>LeNet-5是卷积神经网络的作者Yann LeCun用于MNIST识别任务提出的模型。模型很简单，就是卷积池化层的堆叠，最后加上几层全连接层。我们依样画葫芦，将它运用在文本分类任务中，只是模型的输入不同。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div></pre></td><td class="code"><pre><div class="line">model.add(Convolution1D(<span class="number">256</span>, <span class="number">3</span>, padding=<span class="string">'same'</span>))</div><div class="line">model.add(MaxPool1D(<span class="number">3</span>,<span class="number">3</span>,padding=<span class="string">'same'</span>))</div><div class="line">model.add(Convolution1D(<span class="number">128</span>, <span class="number">3</span>, padding=<span class="string">'same'</span>))</div><div class="line">model.add(MaxPool1D(<span class="number">3</span>,<span class="number">3</span>,padding=<span class="string">'same'</span>))</div><div class="line">model.add(Convolution1D(<span class="number">64</span>, <span class="number">3</span>, padding=<span class="string">'same'</span>))</div><div class="line">model.add(Flatten())</div><div class="line">model.add(Dropout(<span class="number">0.1</span>))</div><div class="line">model.add(Dense(<span class="number">256</span>,activation=<span class="string">'relu'</span>))</div><div class="line">model.add(Dropout(<span class="number">0.1</span>))</div><div class="line">model.add(Dense(num_labels,activation=<span class="string">'softmax'</span>))</div><div class="line"></div><div class="line">model.compile(loss=<span class="string">'categorical_crossentropy'</span>,</div><div class="line">              optimizer=<span class="string">'adam'</span>,</div><div class="line">              metrics=[<span class="string">'accuracy'</span>])</div><div class="line"></div><div class="line">model.fit(X_train_padded_seqs, y_train,</div><div class="line">          batch_size=<span class="number">32</span>,</div><div class="line">          epochs=<span class="number">15</span>,</div><div class="line">          validation_data=(X_test_padded_seqs, y_test))</div></pre></td></tr></table></figure>
</li>
<li><p>TextCNN（这里需要使用Model模式）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div></pre></td><td class="code"><pre><div class="line">main_input = Input(shape=(<span class="number">20</span>,), dtype=<span class="string">'float64'</span>)</div><div class="line"><span class="comment"># 词嵌入（未使用预训练的词向量）</span></div><div class="line">embedder = Embedding(len(vocab) + <span class="number">1</span>, <span class="number">300</span>, input_length = <span class="number">20</span>, weights = [embedding_matrix], trainable = <span class="keyword">False</span>)</div><div class="line">embed = embedder(main_input)</div><div class="line"><span class="comment"># 词窗大小分别为3,4,5</span></div><div class="line">cnn1 = Convolution1D(<span class="number">256</span>, <span class="number">3</span>, padding=<span class="string">'same'</span>, strides = <span class="number">1</span>, activation=<span class="string">'relu'</span>)(embed)</div><div class="line">cnn1 = MaxPool1D(pool_size=<span class="number">4</span>)(cnn1)</div><div class="line">cnn2 = Convolution1D(<span class="number">256</span>, <span class="number">4</span>, padding=<span class="string">'same'</span>, strides = <span class="number">1</span>, activation=<span class="string">'relu'</span>)(embed)</div><div class="line">cnn2 = MaxPool1D(pool_size=<span class="number">4</span>)(cnn2)</div><div class="line">cnn3 = Convolution1D(<span class="number">256</span>, <span class="number">5</span>, padding=<span class="string">'same'</span>, strides = <span class="number">1</span>, activation=<span class="string">'relu'</span>)(embed)</div><div class="line">cnn3 = MaxPool1D(pool_size=<span class="number">4</span>)(cnn3)</div><div class="line"><span class="comment"># 合并三个模型的输出向量</span></div><div class="line">cnn = concatenate([cnn1,cnn2,cnn3], axis=<span class="number">-1</span>)</div><div class="line">flat = Flatten()(cnn)</div><div class="line">drop = Dropout(<span class="number">0.2</span>)(flat)</div><div class="line">main_output = Dense(num_labels, activation=<span class="string">'softmax'</span>)(drop)</div><div class="line">model = Model(inputs = main_input, outputs = main_output)</div></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p>RNN</p>
<ul>
<li><p>LSTM（你也可以换成GRU，经过多次试验GRU的性能较LSTM稍好）</p>
<p>GRU采用与LSTM相似的单元结构用于控制信息的更新与保存，它将遗忘门和输入门合成了一个单一的更新门，最终的模型比标准的 LSTM 模型要简单，也是非常流行的变体。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div></pre></td><td class="code"><pre><div class="line">model = Sequential()</div><div class="line">model.add(Embedding(len(vocab)+<span class="number">1</span>, <span class="number">300</span>, input_length=<span class="number">20</span>))</div><div class="line">model.add(LSTM(<span class="number">256</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.1</span>))</div><div class="line">model.add(Dense(num_labels, activation=<span class="string">'softmax'</span>))</div></pre></td></tr></table></figure>
</li>
<li><p>Bi-GRU</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div></pre></td><td class="code"><pre><div class="line">model = Sequential()</div><div class="line">model.add(Embedding(len(vocab)+<span class="number">1</span>, <span class="number">300</span>, input_length=<span class="number">20</span>))</div><div class="line">model.add(Bidirectional(GRU(<span class="number">256</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.1</span>, return_sequences=<span class="keyword">True</span>)))</div><div class="line">model.add(Bidirectional(GRU(<span class="number">256</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.1</span>)))</div><div class="line">model.add(Dense(num_labels, activation=<span class="string">'softmax'</span>))</div></pre></td></tr></table></figure>
</li>
<li><p>TextRNN</p>
</li>
<li><p>​</p>
</li>
</ul>
</li>
<li><p>CNN+RNN</p>
<ul>
<li><p>串联</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">model = Sequential()</div><div class="line">model.add(Embedding(len(vocab)+<span class="number">1</span>, <span class="number">300</span>, input_length=<span class="number">20</span>))</div><div class="line">model.add(Convolution1D(<span class="number">256</span>, <span class="number">3</span>, padding=<span class="string">'same'</span>, strides = <span class="number">1</span>))</div><div class="line">model.add(Activation(<span class="string">'relu'</span>))</div><div class="line">model.add(MaxPool1D(pool_size=<span class="number">2</span>))</div><div class="line">model.add(GRU(<span class="number">256</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.1</span>, return_sequences = <span class="keyword">True</span>))</div><div class="line">model.add(GRU(<span class="number">256</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.1</span>))</div><div class="line">model.add(Dense(num_labels, activation=<span class="string">'softmax'</span>))</div></pre></td></tr></table></figure>
</li>
<li><p>并联（<a href="http://www.cips-cl.org/static/anthology/CCL-2016/CCL-16-004.pdf" target="_blank" rel="external">论文</a>）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div></pre></td><td class="code"><pre><div class="line">main_input = Input(shape=(<span class="number">20</span>,), dtype=<span class="string">'float64'</span>)</div><div class="line">embed = Embedding(len(vocab)+<span class="number">1</span>, <span class="number">256</span>, input_length=<span class="number">20</span>)(main_input)</div><div class="line">cnn = Convolution1D(<span class="number">256</span>, <span class="number">3</span>, padding=<span class="string">'same'</span>, strides = <span class="number">1</span>, activation=<span class="string">'relu'</span>)(embed)</div><div class="line">cnn = MaxPool1D(pool_size=<span class="number">4</span>)(cnn)</div><div class="line">cnn = Flatten()(cnn)</div><div class="line">cnn = Dense(<span class="number">256</span>)(cnn)</div><div class="line">rnn = Bidirectional(GRU(<span class="number">256</span>, dropout=<span class="number">0.2</span>, recurrent_dropout=<span class="number">0.1</span>))(embed)</div><div class="line">rnn = Dense(<span class="number">256</span>)(rnn)</div><div class="line">con = concatenate([cnn,rnn], axis=<span class="number">-1</span>)</div><div class="line">main_output = Dense(num_labels, activation=<span class="string">'softmax'</span>)(con)</div><div class="line">model = Model(inputs = main_input, outputs = main_output)</div></pre></td></tr></table></figure>
</li>
<li><p>RCNN</p>
<p>​</p>
</li>
</ul>
</li>
<li><p>Attention</p>
</li>
<li><p>FastText（模型很简单，比较复杂的是构造输入数据）</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div><div class="line">11</div><div class="line">12</div><div class="line">13</div><div class="line">14</div><div class="line">15</div><div class="line">16</div><div class="line">17</div><div class="line">18</div><div class="line">19</div><div class="line">20</div><div class="line">21</div><div class="line">22</div><div class="line">23</div><div class="line">24</div><div class="line">25</div><div class="line">26</div><div class="line">27</div><div class="line">28</div><div class="line">29</div><div class="line">30</div><div class="line">31</div><div class="line">32</div><div class="line">33</div><div class="line">34</div><div class="line">35</div><div class="line">36</div><div class="line">37</div></pre></td><td class="code"><pre><div class="line"><span class="comment"># 生成n-gram组合的词(以3为例)</span></div><div class="line">ngram = <span class="number">3</span></div><div class="line"><span class="comment"># 将n-gram词加入到词表</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_ngram</span><span class="params">(sent, ngram_value)</span>:</span></div><div class="line">	<span class="keyword">return</span> set(zip(*[sent[i:] <span class="keyword">for</span> i <span class="keyword">in</span> range(ngram_value)]))</div><div class="line">ngram_set = set()</div><div class="line"><span class="keyword">for</span> sentence <span class="keyword">in</span> X_train_padded_seqs:</div><div class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">2</span>, ngram+<span class="number">1</span>):</div><div class="line">    	set_of_ngram = create_ngram(sentence, i)</div><div class="line">    	ngram_set.update(set_of_ngram)</div><div class="line"><span class="comment"># 给n-gram词汇编码</span></div><div class="line">start_index = len(vocab) + <span class="number">2</span></div><div class="line">token_indice = &#123;v: k + start_index <span class="keyword">for</span> k, v <span class="keyword">in</span> enumerate(ngram_set)&#125; <span class="comment"># 给n-gram词汇编码</span></div><div class="line">indice_token = &#123;token_indice[k]: k <span class="keyword">for</span> k <span class="keyword">in</span> token_indice&#125;</div><div class="line">max_features = np.max(list(indice_token.keys())) + <span class="number">1</span></div><div class="line"><span class="comment"># 将n-gram词加入到输入文本的末端</span></div><div class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_ngram</span><span class="params">(sequences, token_indice, ngram_range)</span>:</span></div><div class="line">    new_sequences = []</div><div class="line">    <span class="keyword">for</span> sent <span class="keyword">in</span> sequences:</div><div class="line">        new_list = sent[:]</div><div class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(new_list) - ngram_range + <span class="number">1</span>):</div><div class="line">            <span class="keyword">for</span> ngram_value <span class="keyword">in</span> range(<span class="number">2</span>, ngram_range + <span class="number">1</span>):</div><div class="line">                ngram = tuple(new_list[i:i + ngram_value])</div><div class="line">                <span class="keyword">if</span> ngram <span class="keyword">in</span> token_indice:</div><div class="line">                    new_list.append(token_indice[ngram])</div><div class="line">        new_sequences.append(new_list)</div><div class="line">    <span class="keyword">return</span> new_sequences</div><div class="line">  </div><div class="line">x_train = add_ngram(X_train_word_ids, token_indice, ngram)</div><div class="line">x_test = add_ngram(X_test_word_ids, token_indice, ngram)</div><div class="line">x_train = pad_sequences(x_train, maxlen=<span class="number">25</span>)</div><div class="line">x_test = pad_sequences(x_test, maxlen=<span class="number">25</span>)</div><div class="line"></div><div class="line">model = Sequential()</div><div class="line">model.add(Embedding(max_features, <span class="number">300</span>, input_length=<span class="number">25</span>))</div><div class="line">model.add(GlobalAveragePooling1D())</div><div class="line">model.add(Dense(num_labels, activation=<span class="string">'softmax'</span>))</div></pre></td></tr></table></figure>
</li>
<li><p>Char-Level</p>
<p>我分别测试了TextCNN和RNN模型采用字符作为输入时的模型性能，发现不尽如人意，究其原因，可能是字级别粒度太细，而且文本是短文本，并不能反映出什么有效的信息。前者能达到0.4勉强及格的准确率，而后者只能达到0.28的准确率。因此，不再尝试对短文本进行字粒度的考证。这里想简单应用字符级别的输入，只需对原始文本稍作改变即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div></pre></td><td class="code"><pre><div class="line">all_sent = [] <span class="comment"># 用于存放新的文本 如 A m y ' s   J i g s a w   S c r a p b o o k</span></div><div class="line"><span class="keyword">for</span> sent <span class="keyword">in</span> title.tolist():</div><div class="line">    new = []</div><div class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> sent:</div><div class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> word:</div><div class="line">            new.append(word)</div><div class="line">        new_sent = <span class="string">" "</span>.join(new)</div><div class="line">    all_sent.append(new_sent)</div></pre></td></tr></table></figure>
<p>​</p>
</li>
<li><p>模型结果比较</p>
<p>​</p>
</li>
</ul>
<h2 id="推文情感分析"><a href="#推文情感分析" class="headerlink" title="推文情感分析"></a>推文情感分析</h2><p>之前我提到其实，这个分类任务并没有什么实际的意义，只是为了熟悉模型，小试牛刀，接下来我们来试试更大的数据集，比如推文的情感分析任务。</p>
<h2 id="京东评论"><a href="#京东评论" class="headerlink" title="京东评论"></a>京东评论</h2><p>我们再来试试中文的文本分类。中文天生有个坑需要去跨越，那就是分词，对于一般的文本，现有的分词工具已经能够出色地完成任务了。而面对网络文本，目前还没有看到有效的解决方案，这篇<a href="http://www.aclweb.org/anthology/N16-1030" target="_blank" rel="external">论文</a>尝试使用Bi-LSTM+CRF的方法来实现深度学习模型分词器，有空可以来研究下。所以，本文目前还是使用的主流的分词工具结巴分词。</p>
<h2 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h2><p>我们来回顾下，我们至此学到了哪些内容。</p>
<ul>
<li>文本分类任务的处理流程</li>
<li>传统机器学习模型的处理方法（Baseline：TF-IDF + 分类器）</li>
<li>常见的深度学习模型（CNN，RNN，Attention）</li>
<li>历年经典论文中的模型复现（TextCNN，RCNN，HAN等）</li>
<li>实战（多分类的小数据集，推文情感分析，京东评论好中差评）</li>
</ul>
<p>最后给大家推荐一些高质量的学习资源：</p>
<ul>
<li>NLP大牛 Chris Manning 和 Richard Socher 在 Stanford 合开的课程CS224d【<a href="http://cs224d.stanford.edu/syllabus.html" target="_blank" rel="external">课程链接</a>】（一直静下心来认真学习这门课程 T T）</li>
<li>Google Brain成员Christopher Olah大神的理论<a href="http://colah.github.io/" target="_blank" rel="external">博客</a></li>
<li>另一位Google Brain成员Denny Britz大神关于NLP的博客 <a href="http://www.wildml.com/" target="_blank" rel="external">WILDML</a></li>
<li>MOOC课程资源：优达学城深度学习基石纳米<a href="https://cn.udacity.com/dlnd" target="_blank" rel="external">学位</a>和吴恩达三大项目之一<a href="https://www.deeplearning.ai/" target="_blank" rel="external">deeplearning.ai</a></li>
</ul>
<p>希望大家能够有所收获，第一次自己动手码字，有很多论文和模型的理解还不够深刻！关于代码部分，我接下来会整理好放到github上，你们也能在我的主页找到我的github地址。欢迎大家向我提出宝贵的意见！如果你喜欢我的文章，欢迎转发，注明转发来源即可，链接二维码在此 :D</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-30/68491740.jpg" alt=""></p>

          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview sidebar-panel sidebar-panel-active">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/dabai.jpg"
               alt="Yilei Wang" />
          <p class="site-author-name" itemprop="name">Yilei Wang</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/stevewyl" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="stevewyl@163.com" target="_blank" title="E-Mail">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                    
                      E-Mail
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yilei Wang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">主题 &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
    <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>

  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  




	





  





  








  





  

  
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.4.js"></script>
  <script>AV.initialize("3t1EHko0Ctbvuk2O9gAvOex5-9Nh9j0Va", "7yP9fKdHsXruhlcqv9vwnGES");</script>
  <script>
    function showTime(Counter) {
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var time = item.get('time');
            var element = document.getElementById(url);

            $(element).find(COUNT_CONTAINER_REF).text(time);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("time");
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('time'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);
            newcounter.set("url", url);
            newcounter.set("time", 1);
            newcounter.save(null, {
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('time'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        addCount(Counter);
      } else if ($('.post-title-link').length > 1) {
        showTime(Counter);
      }
    });
  </script>



  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
