<!DOCTYPE html>



  


<html class="theme-next muse use-motion" lang="zh-Hans">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.2" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="文本分类,Keras," />








  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=5.1.2" />






<meta name="description" content="写在前面从优达DLND毕业后，一直想自己动手做点什么来着，互助班的导师也鼓励自己动手写点心得体验啥的。之前一直没怎么观看Youtube网红Siraj老师的课程视频，他每个视频最后都会有一个编程挑战。于是，想着先从自己熟悉的内容着手吧，Siraj老师第三周的编程挑战是做一个多类别的文本分类器，链接在此：Github，那就来试试吧。除了想自己练练手外，也顺便把模型都好好梳理一遍。为了给自己增加些难度，">
<meta name="keywords" content="文本分类,Keras">
<meta property="og:type" content="article">
<meta property="og:title" content="Keras之文本分类实现">
<meta property="og:url" content="https://stevewyl.github.io/2017/08/27/keras/index.html">
<meta property="og:site_name" content="DL炼丹房">
<meta property="og:description" content="写在前面从优达DLND毕业后，一直想自己动手做点什么来着，互助班的导师也鼓励自己动手写点心得体验啥的。之前一直没怎么观看Youtube网红Siraj老师的课程视频，他每个视频最后都会有一个编程挑战。于是，想着先从自己熟悉的内容着手吧，Siraj老师第三周的编程挑战是做一个多类别的文本分类器，链接在此：Github，那就来试试吧。除了想自己练练手外，也顺便把模型都好好梳理一遍。为了给自己增加些难度，">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/57232353.jpg">
<meta property="og:image" content="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/19849125.jpg">
<meta property="og:image" content="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/73075953.jpg">
<meta property="og:image" content="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/30339409.jpg">
<meta property="og:image" content="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/18106498.jpg">
<meta property="og:image" content="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/54396153.jpg">
<meta property="og:image" content="http://ovcg0j4b2.bkt.clouddn.com/17-8-29/69730967.jpg">
<meta property="og:image" content="http://ovcg0j4b2.bkt.clouddn.com/17-8-29/74136352.jpg">
<meta property="og:image" content="http://ovcg0j4b2.bkt.clouddn.com/17-8-29/75206008.jpg">
<meta property="og:image" content="http://ovcg0j4b2.bkt.clouddn.com/17-8-29/18150827.jpg">
<meta property="og:updated_time" content="2017-08-28T17:14:58.291Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Keras之文本分类实现">
<meta name="twitter:description" content="写在前面从优达DLND毕业后，一直想自己动手做点什么来着，互助班的导师也鼓励自己动手写点心得体验啥的。之前一直没怎么观看Youtube网红Siraj老师的课程视频，他每个视频最后都会有一个编程挑战。于是，想着先从自己熟悉的内容着手吧，Siraj老师第三周的编程挑战是做一个多类别的文本分类器，链接在此：Github，那就来试试吧。除了想自己练练手外，也顺便把模型都好好梳理一遍。为了给自己增加些难度，">
<meta name="twitter:image" content="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/57232353.jpg">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '5.1.2',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="https://stevewyl.github.io/2017/08/27/keras/"/>





  <title>Keras之文本分类实现 | DL炼丹房</title><!-- hexo-inject:begin --><!-- hexo-inject:end -->
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">DL炼丹房</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="https://stevewyl.github.io/2017/08/27/keras/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Yilei Wang">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/uploads/dabai.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="DL炼丹房">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">Keras之文本分类实现</h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2017-08-27T19:41:01+08:00">
                2017-08-27
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/深度学习/" itemprop="url" rel="index">
                    <span itemprop="name">深度学习</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
              <span class="post-comments-count">
                <span class="post-meta-divider">|</span>
                <span class="post-meta-item-icon">
                  <i class="fa fa-comment-o"></i>
                </span>
                <a href="/2017/08/27/keras/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count disqus-comment-count"
                        data-disqus-identifier="2017/08/27/keras/" itemprop="commentCount"></span>
                </a>
              </span>
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <h2 id="写在前面"><a href="#写在前面" class="headerlink" title="写在前面"></a>写在前面</h2><p>从优达DLND毕业后，一直想自己动手做点什么来着，互助班的导师也鼓励自己动手写点心得体验啥的。之前一直没怎么观看Youtube网红Siraj老师的课程视频，他每个视频最后都会有一个编程挑战。于是，想着先从自己熟悉的内容着手吧，Siraj老师第三周的编程挑战是做一个多类别的文本分类器，链接在此：<a href="https://github.com/llSourcell/How_to_do_Sentiment_Analysis" target="_blank" rel="external">Github</a>，那就来试试吧。除了想自己练练手外，也顺便把模型都好好梳理一遍。为了给自己增加些难度，是否有可能把过去几年内那些大牛们论文中的模型复现出来呢？阅读这篇文章，需要你对深度学习的模型有一个基础的了解哦！</p>
<h2 id="文本多分类"><a href="#文本多分类" class="headerlink" title="文本多分类"></a>文本多分类</h2><p>首先我们来看下数据集长什么样子吧 :P Let‘s get started!</p>
<p>我们使用pandas来加载数据，数据集来自IGN.com，收集了过去20年各大游戏厂商发布的游戏数据，如发布日期，发布平台，游戏评价等变量，这里有一篇关于这个数据集很不错的分析教程 <a href="https://www.kaggle.com/ash316/20-years-of-games-analysis" target="_blank" rel="external">Kaggle</a> 。而我们现在想分析下游戏名与用户评价之间的关系，看上去并不合理，我们姑且按照Siraj老师的任务来试试。于是，游戏名作为文本变量将作为模型的输入X，而用户评价词作为文本类别Y。<br>然后来看看各个类别的数量，为了避免类别样本数的不平衡，我们这里把关于评价为Disaster的游戏去除。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div></pre></td><td class="code"><pre><div class="line">df = pd.read_csv(<span class="string">'ign.csv'</span>).iloc[:, <span class="number">1</span>:<span class="number">3</span>]</div><div class="line">df.score_phrase.value_counts()</div><div class="line">df = df[df.score_phrase != <span class="string">'Disaster'</span>]</div></pre></td></tr></table></figure>
<p>首先我们先来试试传统机器学习模型对文本分类任务常见的做法吧</p>
<h3 id="传统文本分类方法"><a href="#传统文本分类方法" class="headerlink" title="传统文本分类方法"></a>传统文本分类方法</h3><ul>
<li><p>词袋模型</p>
<p>由于计算机只能处理数字型的变量，并不能直接把文本丢给计算机然后让它告诉我们这段文字的类别。于是，我们需要对词进行one-hot编码。假设我们总共有N个词，然后对词进行索引编码并构造一个N维零向量，如果这个文本中的某些词出现，就在该词索引值位置标记为1，表示这个文本包含这个词。于是，我们会得到如下类似的向量</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">( 0, 0, 1, 0, .... , 1, ... 0, 0, 1, 0)</div></pre></td></tr></table></figure>
<p>但是，一般来说词库量至少都是百万级别，因此词袋模型有个两个最大的问题：高维度、高稀疏性。这种表示方法还存在一个重要的问题就是”词汇鸿沟”现象：任意两个词之间都是孤立的。光从这两个向量中看不出两个词是否有关系。</p>
</li>
<li><p>共现矩阵</p>
<p>为了使用上下文来表示单词间的关系，也有人提出使用基于窗口大小的共现矩阵，但仍然存在数据维度大稀疏的问题。</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/57232353.jpg" alt=""></p>
</li>
<li><p>TF-IDF</p>
<p>TF-IDF 用以评估一字词对于一个文档集或一个语料库中的其中一份文档的重要程度，是一种计算特征权重的方法。核心思想即，字词的重要性随着它在文档中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。有效地规避了那些高频却包含很少信息量的词。我们这里也是用TF-IDF 对文本变量进行特征提取。</p>
</li>
<li><p>分类器</p>
<p>分类器就是常见的机器学习分类模型了，常用的有以下两种，这里我不再赘述这两个模型的原理了。</p>
<ul>
<li>朴素贝叶斯：从垃圾邮件识别应用开始被广泛使用</li>
<li>支持向量机：这篇<a href="http://www.linkedin.com/pulse/please-explain-support-vector-machines-svm-like-i-am-5-joni-salminen" target="_blank" rel="external">文章</a> 很通俗地解释了SVM的工作原理 </li>
</ul>
</li>
</ul>
<p>使用Scikit-Learn库能够傻瓜似的来实现你的机器学习模型，我们这里使用TfidfVectorizer函数对文本进行特征处理，并去除停用词，模型有多类别朴素贝叶斯和线性SVM分类器。结果很不令人满意，NB模型结果稍好，准确率为28%，领先SVM 1%。下面我们来看看深度学习模型强大的性能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><div class="line">1</div><div class="line">2</div><div class="line">3</div><div class="line">4</div><div class="line">5</div><div class="line">6</div><div class="line">7</div><div class="line">8</div><div class="line">9</div><div class="line">10</div></pre></td><td class="code"><pre><div class="line">vect = TfidfVectorizer(stop_words=<span class="string">'english'</span>, token_pattern=<span class="string">r'\b\w&#123;2,&#125;\b'</span>,</div><div class="line">                       min_df=<span class="number">1</span>, max_df=<span class="number">0.1</span>, ngram_range=(<span class="number">1</span>,<span class="number">2</span>))</div><div class="line">mnb = MultinomialNB(alpha=<span class="number">2</span>)</div><div class="line">svm = SGDClassifier(loss=<span class="string">'hinge'</span>, penalty=<span class="string">'l2'</span>, alpha=<span class="number">1e-3</span>, n_iter=<span class="number">5</span>, random_state=<span class="number">42</span>)</div><div class="line">mnb_pipeline = make_pipeline(vect, mnb)</div><div class="line">svm_pipeline = make_pipeline(vect, svm)</div><div class="line">mnb_cv = cross_val_score(mnb_pipeline, title, label, scoring=<span class="string">'accuracy'</span>, cv=<span class="number">10</span>, n_jobs=<span class="number">-1</span>)</div><div class="line">svm_cv = cross_val_score(svm_pipeline, title, label, scoring=<span class="string">'accuracy'</span>, cv=<span class="number">10</span>, n_jobs=<span class="number">-1</span>)</div><div class="line">print(<span class="string">'\nMultinomialNB Classifier\'s Accuracy: %0.5f\n'</span> % mnb_cv.mean())</div><div class="line">print(<span class="string">'\nSVM Classifier\'s Accuracy: %0.5f\n'</span> % svm_cv.mean())</div></pre></td></tr></table></figure>
<h3 id="走进NLP和DL"><a href="#走进NLP和DL" class="headerlink" title="走进NLP和DL"></a>走进NLP和DL</h3><p>传统方法对于文本的特征表达能力很弱，神经网络同样不擅长处理这样高维度高稀疏性的数据，因此我们需要对文本做进一步的特征处理，这里就要讲到词嵌入的概念了。深度学习模型中，一个单词常常用一个低维且稠密的向量来表示，如下所示：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><div class="line">1</div></pre></td><td class="code"><pre><div class="line">( 0.286, 0.792, -0.177, -0.107, .... , 0.109, ... 0.349, 0.271, -0.642)</div></pre></td></tr></table></figure>
<ul>
<li><p>词向量</p>
<p>主流的词嵌入实现方法有Mikolov的<a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="external">Word2Vec</a>和斯坦福大学的<a href="https://nlp.stanford.edu/pubs/glove.pdf" target="_blank" rel="external">Glove</a>，也有人做过<a href="https://rare-technologies.com/making-sense-of-word2vec/" target="_blank" rel="external">实验</a>来比较这两种方法的优劣，并没有多大的差异。Word2Vec是基于预测的词向量模型，简单来说，就是给定一个词，去预测这个词周围可能出现的词，或者给定一些词来确定位于中心位置的词。而Glove是基于统计方法的，通过对词-词共现矩阵里的非零元素进行训练。总体来说，Word2Vec使用的是局部信息，而Glove使用的是全局信息，因此前者训练起来更慢但不占用内存，而Glove通过更多的计算资源换取训练速度上的提升。具体的实现细节可以参考这两篇论文。本文中将使用预训练的Glove300维词向量和由自己文本生成的词向量。</p>
</li>
</ul>
<p>为了快速实现模型，这篇文章中将使用Keras（TensorFlow的高级API）来完成。</p>
<p>我们先来了解一些基础的深度学习模型吧！</p>
<ul>
<li><p>CNN </p>
<p>深度学习入门必学的两大模型之一卷积神经网络。首先我们来理解下什么是卷积操作？卷积，你可以把它想象成一个应用在矩阵上的滑动窗口函数。下图中左边的矩阵表示的是一张黑白图像。每个方格代表了一个像素，0表示黑色，1表示白色。这个滑动窗口称作kernel或者filter。这里我们使用的是一个3*3的filter，将它的值和与其对应的原图像矩阵进行相乘，然后再将它们相加。这样我们在整个原图矩阵上滑动filter来遍历所有像素后得到一个完整的卷积特征。</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/19849125.jpg" alt=""></p>
<p>卷积网络也就是对输入样本进行多次卷积操作，提取数据中的局部位置的特征，然后再拼接池化层（图中的Pooling层）做进一步的降维操作，最后与全连接层拼接完成对输入样本的全新的特征构造，将新的特征向量输送给分类器（以图片分类为例）进行预测分类。我们可以把CNN类比N-gram模型，N-gram也是基于词窗范围这种局部的方式对文本进行特征提取，与CNN的做法很类似，在下文中，我们再来看看如何运用CNN对文本数据进行建模。</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/73075953.jpg" alt=""></p>
<p>卷积网络开始崭露头角是在CV领域，2012年的ImageNet竞赛中，大大降低了图片分类的错误率。为什么CNN在计算机视觉领域这么厉害呢？直观的感受就是：</p>
<ul>
<li>它能够学习识别基本的直线，曲线，然后是形状，点块，然后是图片中更复杂的物体。最终CNN分类器把这些大的，复杂的物体综合起来识别图片</li>
<li>在下图中的例子中，可以看作这样的层级关系：<ul>
<li>简单的形状，如椭圆，暗色圆圈</li>
<li>复杂的物体（简单形状的组合），例如眼睛，鼻子，毛发</li>
<li>狗的整体（复杂物体的组合）</li>
</ul>
</li>
</ul>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/30339409.jpg" alt=""></p>
</li>
<li><p>RNN</p>
<p>而循环网络与CNN不同的是，CNN学习空间位置上局部位置的特征表示，而RNN学习的是时间顺序上的特征，用来处理序列数据，如股价，文本等。RNN之所以称为循环神经网路，即一个序列当前的输出与前面的输出也有关。具体的表现形式为网络会对前面的信息进行记忆并应用于当前输出的计算中，即隐藏层之间的节点是相互连接的，并且隐藏层的输入不仅包括输入层的输出还包括上一时刻隐藏层的输出。</p>
<p>就像我们说话一样，我们不能把说的话倒过来表示，这样会变得毫无意义，并不会明白你在说什么，也就是说文本中的每个词是包含顺序信息的，由此可以使用RNN对文本数据进行建模。<br><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/18106498.jpg" alt=""></p>
<p>但是，随着时间的不断增加，你的隐藏层一次又一次地乘以权重W。假如某个权重w是一个接近于0或者大于1的数，随着乘法次数的增加，这个权重值会变得很小或者很大，造成反向传播时梯度计算变得很困难，造成梯度爆炸或者梯度消失的情况，模型难以训练。也就是说一般的RNN模型对于长时间距离的信息记忆很差，比如人老了会忘记某件很久发生的事情一样，于是，LSTM和GRU 应运而生。LSTM与GRU很相似，以LSTM为例。</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-27/54396153.jpg" alt=""></p>
<p>LSTM又称为长短期记忆网络，LSTM 单元乍看起来很复杂。关键的新增部分就在于标记为 C 的单元状态。在这个单元中，有四个显示为黄色框的网络层，每个层都有自己的权重，如以 σ 标记的层是 sigmoid 层。这些红圈表示逐点或逐元素操作。单元状态在通过 LSTM 单元时几乎没有交互，使得大部分信息得以保留，单元状态仅通过这些控制门（gate）进行修改。第一个控制门是遗忘门，用来决定我们会从单元状态中丢弃什么信息。第二个门是更新们，用以确定什么样的新信息被存放到单元状态中。最后一个门是输出门，我们需要确定输出什么样的值。总结来说 LSTM 单元由单元状态和一堆用于更新信息的控制门组成，让信息部分传递到隐藏层状态。更直观的来讲，把LSTM看作是一部电影，可以把单元状态看作是剧情主线，而随着剧情的发展，有些不必要的事件会被遗忘，而一些更加影响主线的剧情会被加入到单元状态中来，不断更新剧情然后输出新的剧情发展。</p>
</li>
<li><p>Attention机制</p>
<p>基于Attention的模型在NLP领域首先被应用于自然语言生成问题中，用于改进机器翻译任务的性能。我们这里也以机器翻译为例来解释下注意力机制的原理。我们可以把翻译任务是一个序列向另一个序列转换的过程。</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-29/69730967.jpg" alt=""></p>
<p>上图就是Seq2Seq模型的基本结构，由编码器（Encoder）和解码器（Decoder）组成。编码器负责将输入的单词按顺序进行信息提取，在最后一步生成的隐藏状态即固定长度的句子的特征向量。然后解码器从这个句子向量中获取信息对文本进行翻译。由于解码器的主要信息来源就是最后一步的隐藏状态，这个h3向量必须尽可能地包含句子的所有必要的信息。这个向量说白了就是句子嵌入（类比词嵌入）。假如我们需要翻译的文本不是很长，这个模型已经能达到很不错的性能。假如我们现在要翻译一句超过50个单词的句子，似乎这个模型很难再hold住，即使你在训练的时候使用了LSTM去提取句子特征，去尽可能保留过去的记忆，但还是达不到想要的结果。</p>
<p>而注意力机制恰恰是为了解决长距离依赖的问题，我们不再需要固定长度的句子向量，而是让解码器自己去输入文本中寻找想要关注的被翻译文本。比如把”I am learning deep learning model“成中文时，我们让解码器去与输入文本中的词对齐，翻译deep的时候去关注deep这个词，而不是平等对待每个有可能的词，找到与输入文本相对应的相同语义的词，而不再是对句子进行特征提取。</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-29/74136352.jpg" alt=""></p>
<p>上图我们可以看到，解码器在翻译下一个词时，需要依赖之前已经翻译好的文本和与输入文本相对齐的那个词。简单描述的话，用解码器t时刻的隐藏状态去和输入文本中的每个单词对应的隐藏状态去比对，通过某个函数f去计算带翻译的单词yi与每个输入单词对齐的可能性。而编码器由Bi-LSTM模型组成。不同的语言的f函数可能会有差别，就像中文和英文，语法结构差异很大，很难按顺序单词一一对齐。由此可以得出结论，注意力机制的核心思想是在翻译每个目标词（或对文本进行分类时）所用的上下文是不同的，这样的考虑显然是更合理的。具体实现请见这篇<a href="https://arxiv.org/pdf/1409.0473v7.pdf" target="_blank" rel="external">论文</a>。而如何将注意力机制运用到文本分类中来，下文会介绍。</p>
</li>
</ul>
<h3 id="深度学习文本分类模型"><a href="#深度学习文本分类模型" class="headerlink" title="深度学习文本分类模型"></a>深度学习文本分类模型</h3><ul>
<li><p>TextCNN</p>
<p>这是CNN首次被应用于文本分类任务的开山之作，可以说，之后很多论文都是基于此进行拓展的。它是由<a href="https://arxiv.org/abs/1408.5882" target="_blank" rel="external">Yoon Kim</a>于2014年提出的，你可以在github上找到各种不同深度学习框架对于这个模型的实现。下面我们来细细品读这篇论文吧。</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-29/75206008.jpg" alt=""></p>
<p>上图很好地诠释了模型的框架。假设我们有一句句子需要对其进行分类。句子中每个词是由n维词向量组成的，也就是说输入矩阵大小为m*n，其中m为句子长度。CNN需要对输入样本进行卷积操作，对于文本数据，filter不再横向滑动，仅仅是向下移动，有点类似于N-gram在提取词与词间的局部相关性。图中共有三种步长策略，分别是2,3,4，每个步长都有两个filter（实际训练时filter数量会很多）。在不同词窗上应用不同filter，最终得到6个卷积后的向量。然后对每一个向量进行最大化池化操作并拼接各个池化值，最终得到这个句子的特征表示，将这个句子向量丢给分类器进行分类，完成整个流程。</p>
</li>
<li><p>Bi-LSTM</p>
</li>
<li><p>TextCNN with K-pooling</p>
</li>
<li><p>CLSTM </p>
</li>
<li><p>RCNN</p>
</li>
<li><p>FastText</p>
<p>FastText是Facebook于2016年发表的论文中提出的一种简单快速实现的 文本分类模型。可能你已经被前面那些复杂的模型搞得七荤八素了，那么这个模型你很快地理解，令人意外的是，它的性能并不差。输入变量是经过embedding的词向量，这里的隐藏层只是一个简单的平均池化层，然后把这个池化过的向量丢给softmax分类器就完成了。另外，这里的X并不仅仅是单个单词，也可以加入N-gram组合的词作为输入的一部分，文中将2-元和3-元的特征也加入到了模型中。本文的思想在于通过简单的特征线性组合就可以达到不错的分类性能，我们可以把fasttext当作是工业界一种快速实现模型的产物。</p>
<p><img src="http://ovcg0j4b2.bkt.clouddn.com/17-8-29/18150827.jpg" alt=""></p>
</li>
<li><p>HAN</p>
</li>
<li><p>Attention Model</p>
<ul>
<li>本文只涉及单句，因此构不成文档，只需要 word-level 这一层的注意力即可。加入Attention之后最大的好处自然是能够直观的解释各个句子和词对分类类别的重要性。</li>
</ul>
</li>
</ul>
<h2 id="推文情感分析"><a href="#推文情感分析" class="headerlink" title="推文情感分析"></a>推文情感分析</h2>
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/文本分类/" rel="tag"># 文本分类</a>
          
            <a href="/tags/Keras/" rel="tag"># Keras</a>
          
        </div>
      

      
      
      

      

      
      
    </footer>
  </div>
  
  
  
  </article>



    <div class="post-spread">
      
    </div>
  </div>


          </div>
          


          
  <div class="comments" id="comments">
    
      <div id="disqus_thread">
        <noscript>
          Please enable JavaScript to view the
          <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a>
        </noscript>
      </div>
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/uploads/dabai.jpg"
               alt="Yilei Wang" />
          <p class="site-author-name" itemprop="name">Yilei Wang</p>
           
              <p class="site-description motion-element" itemprop="description"></p>
          
        </div>
        <nav class="site-state motion-element">

          
            <div class="site-state-item site-state-posts">
            
              <a href="/archives/">
            
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            
            
            <div class="site-state-item site-state-categories">
              
                <span class="site-state-item-count">1</span>
                <span class="site-state-item-name">分类</span>
              
            </div>
          

          
            
            
            <div class="site-state-item site-state-tags">
              
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">标签</span>
              
            </div>
          

        </nav>

        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/stevewyl" target="_blank" title="GitHub">
                  
                    <i class="fa fa-fw fa-github"></i>
                  
                    
                      GitHub
                    
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="stevewyl@163.com" target="_blank" title="E-Mail">
                  
                    <i class="fa fa-fw fa-envelope"></i>
                  
                    
                      E-Mail
                    
                </a>
              </span>
            
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#写在前面"><span class="nav-number">1.</span> <span class="nav-text">写在前面</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#文本多分类"><span class="nav-number">2.</span> <span class="nav-text">文本多分类</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#传统文本分类方法"><span class="nav-number">2.1.</span> <span class="nav-text">传统文本分类方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#走进NLP和DL"><span class="nav-number">2.2.</span> <span class="nav-text">走进NLP和DL</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#深度学习文本分类模型"><span class="nav-number">2.3.</span> <span class="nav-text">深度学习文本分类模型</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#推文情感分析"><span class="nav-number">3.</span> <span class="nav-text">推文情感分析</span></a></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2017</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Yilei Wang</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动</div>

  <span class="post-meta-divider">|</span>

  <div class="theme-info">主题 &mdash; <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">NexT.Muse</a> v5.1.2</div>


        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  


  











  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>

  
  <script type="text/javascript" src="/lib/canvas-nest/canvas-nest.min.js"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.2"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.2"></script>



  
  

  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.2"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.2"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.2"></script>



  


  

    
      <script id="dsq-count-scr" src="https://.disqus.com/count.js" async></script>
    

    
      <script type="text/javascript">
        var disqus_config = function () {
          this.page.url = 'https://stevewyl.github.io/2017/08/27/keras/';
          this.page.identifier = '2017/08/27/keras/';
          this.page.title = 'Keras之文本分类实现';
        };
        var d = document, s = d.createElement('script');
        s.src = 'https://.disqus.com/embed.js';
        s.setAttribute('data-timestamp', '' + +new Date());
        (d.head || d.body).appendChild(s);
      </script>
    

  




	





  





  








  





  

  

  

  
  
    <script type="text/x-mathjax-config">
      MathJax.Hub.Config({
        tex2jax: {
          inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
          processEscapes: true,
          skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
        }
      });
    </script>

    <script type="text/x-mathjax-config">
      MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax(), i;
        for (i=0; i < all.length; i += 1) {
          all[i].SourceElement().parentNode.className += ' has-jax';
        }
      });
    </script>
    <script type="text/javascript" src="//cdn.bootcss.com/mathjax/2.7.1/latest.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
  


  

  

</body>
</html>
